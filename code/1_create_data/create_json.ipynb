{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import random\n",
    "# import glob\n",
    "\n",
    "# def create_dataset_json_multisite(root_dir, output_folder):\n",
    "#     # Get a list of all image files in the root directory\n",
    "#     subject_list = glob.glob(os.path.join(root_dir, '*.png'))  # Update the file extension if necessary\n",
    "    \n",
    "#     # Group hospitals\n",
    "#     hospital_groups = {\n",
    "#         '18': 'hospital18.json',\n",
    "#         '43': 'hospital43.json',\n",
    "#         '55': 'hospital55.json'\n",
    "#     }\n",
    "    \n",
    "#     for hospital_ids, output_filename in hospital_groups.items():\n",
    "#         # Filter images for the current hospital group\n",
    "#         filtered_images = [file for file in subject_list if any(f\"_hospital_{hospital_id}_\" in file for hospital_id in hospital_ids.split(','))]\n",
    "        \n",
    "#         # Shuffle the images and split them into train and val sets (70-30 split)\n",
    "#         random.shuffle(filtered_images)\n",
    "#         num_validation = int(len(filtered_images) * 0.3)\n",
    "#         train_data = filtered_images[:-num_validation]\n",
    "#         val_data = filtered_images[-num_validation:]\n",
    "        \n",
    "#         # Create the dataset dictionary\n",
    "#         dataset = {\"train\": [], \"val\": []}\n",
    "#         for file in train_data:\n",
    "#             dataset[\"train\"].append({\n",
    "#                 \"image_path\": file,\n",
    "#                 \"hospital_id\": hospital_ids,\n",
    "#                 \"label\": os.path.basename(file).split('_')[4].split('.')[0]  # Extract label information, adjust the index accordingly\n",
    "#             })\n",
    "#         for file in val_data:\n",
    "#             dataset[\"val\"].append({\n",
    "#                 \"image_path\": file,\n",
    "#                 \"hospital_id\": hospital_ids,\n",
    "#                 \"label\": os.path.basename(file).split('_')[4].split('.')[0]  # Extract label information, adjust the index accordingly\n",
    "#             })\n",
    "\n",
    "#         # Write the dataset to JSON file\n",
    "#         with open(os.path.join(output_folder, output_filename), 'w') as json_file:\n",
    "#             json.dump(dataset, json_file, indent=4)\n",
    "            \n",
    "#         print(f\"JSON file '{output_filename}' created for hospital IDs: {hospital_ids}\")\n",
    "\n",
    "# # Example usage\n",
    "# root_directory = \"/local/data1/honzh073/data/8bit_down224\"\n",
    "# output_folder = \"/local/data1/honzh073/local_repository/FL/code/5_create_json_files\"\n",
    "\n",
    "# create_dataset_json_multisite(root_directory, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create different json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file 'hospital22_3.json' created for training and testing sets.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_dataset_json(csv_path, output_folder):\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Get unique PatientIDs\n",
    "    patient_ids = df['PatientID'].unique()\n",
    "\n",
    "    # Split data into train and test sets (70-30 split)\n",
    "    random_state = 3\n",
    "    train_patient_ids, val_patient_ids = train_test_split(patient_ids, test_size=0.3, random_state=random_state)\n",
    "\n",
    "    # Filter data for the train and test sets\n",
    "    train_data = df[df['PatientID'].isin(train_patient_ids)]\n",
    "    val_data = df[df['PatientID'].isin(val_patient_ids)]\n",
    "\n",
    "    # Create the dataset dictionary\n",
    "    dataset = {\"train\": [], \"val\": []}\n",
    "    for _, row in train_data.iterrows():\n",
    "        dataset[\"train\"].append({\n",
    "            \"HospitalID\": row['HospitalID'],\n",
    "            \"PatientID\": row['PatientID'],\n",
    "            \"ImageID\": row['ImageID'],\n",
    "            \"ImagePath\": row['ImagePath'],\n",
    "            \"Label\": row['Label']\n",
    "        })\n",
    "    for _, row in val_data.iterrows():\n",
    "        dataset[\"val\"].append({\n",
    "            \"HospitalID\": row['HospitalID'],\n",
    "            \"PatientID\": row['PatientID'],\n",
    "            \"ImageID\": row['ImageID'],\n",
    "            \"ImagePath\": row['ImagePath'],\n",
    "            \"Label\": row['Label']\n",
    "        })\n",
    "\n",
    "    # Write the dataset to a JSON file\n",
    "    output_filename = \"hospital22_3.json\"\n",
    "    output_path = os.path.join(output_folder, output_filename)\n",
    "    with open(output_path, 'w') as json_file:\n",
    "        json.dump(dataset, json_file, indent=4)\n",
    "\n",
    "    print(f\"JSON file '{output_filename}' created for training and testing sets.\")\n",
    "\n",
    "# Example usage\n",
    "csv_path = \"/local/data1/honzh/local_repository/FL/code/0_stats_pyradiomics/single_hospital_csv/hospital22.csv\"\n",
    "output_folder = \"/local/data1/honzh/local_repository/FL/code/1_create_data/data2\"\n",
    "\n",
    "create_dataset_json(csv_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train集合:\n",
      "患者ID数量: 17\n",
      "AFF标签的图片数量: 30 (44.12%)\n",
      "NFF标签的图片数量: 38 (55.88%)\n",
      "\n",
      "Val集合:\n",
      "患者ID数量: 8\n",
      "AFF标签的图片数量: 14 (48.28%)\n",
      "NFF标签的图片数量: 15 (51.72%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def print_statistics(json_file_path, dataset_type):\n",
    "    # 读取JSON文件\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # 将JSON数据转换为DataFrame\n",
    "    df = pd.DataFrame(data[dataset_type])\n",
    "\n",
    "    # 统计患者ID数量\n",
    "    num_patients = df['PatientID'].nunique()\n",
    "\n",
    "    # 统计AFF和NFF标签的图片数量\n",
    "    num_aff_images = df[df['Label'] == 'AFF'].shape[0]\n",
    "    num_nff_images = df[df['Label'] == 'NFF'].shape[0]\n",
    "\n",
    "    # 计算比例\n",
    "    ratio_aff = num_aff_images / len(df)\n",
    "    ratio_nff = num_nff_images / len(df)\n",
    "\n",
    "    # 打印统计信息和比例\n",
    "    print(f\"{dataset_type.capitalize()}集合:\")\n",
    "    print(f\"患者ID数量: {num_patients}\")\n",
    "    print(f\"AFF标签的图片数量: {num_aff_images} ({ratio_aff:.2%})\")\n",
    "    print(f\"NFF标签的图片数量: {num_nff_images} ({ratio_nff:.2%})\")\n",
    "    print()\n",
    "\n",
    "# Example usage\n",
    "json_file_path = \"/local/data1/honzh/local_repository/FL/code/1_create_data/data2/hospital22_3.json\"\n",
    "print_statistics(json_file_path, 'train')\n",
    "print_statistics(json_file_path, 'val')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分层抽样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file 'hospital18_3.json' created for training and testing sets.\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import random\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# def create_dataset_json(csv_path, output_folder):\n",
    "#     # Read CSV file\n",
    "#     df = pd.read_csv(csv_path)\n",
    "\n",
    "#     # Split data into train and test sets with stratified sampling\n",
    "#     random_state = 2\n",
    "#     train_data, val_data = train_test_split(df, test_size=0.3, random_state=random_state, stratify=df['Label'])\n",
    "\n",
    "#     # Create the dataset dictionary\n",
    "#     dataset = {\"train\": [], \"val\": []}\n",
    "#     for _, row in train_data.iterrows():\n",
    "#         dataset[\"train\"].append({\n",
    "#             \"HospitalID\": row['HospitalID'],\n",
    "#             \"PatientID\": row['PatientID'],\n",
    "#             \"ImageID\": row['ImageID'],\n",
    "#             \"ImagePath\": row['ImagePath'],\n",
    "#             \"Label\": row['Label']\n",
    "#         })\n",
    "#     for _, row in val_data.iterrows():\n",
    "#         dataset[\"val\"].append({\n",
    "#             \"HospitalID\": row['HospitalID'],\n",
    "#             \"PatientID\": row['PatientID'],\n",
    "#             \"ImageID\": row['ImageID'],\n",
    "#             \"ImagePath\": row['ImagePath'],\n",
    "#             \"Label\": row['Label']\n",
    "#         })\n",
    "\n",
    "#     # Write the dataset to a JSON file\n",
    "#     output_filename = \"hospital18_3.json\"\n",
    "#     output_path = os.path.join(output_folder, output_filename)\n",
    "#     with open(output_path, 'w') as json_file:\n",
    "#         json.dump(dataset, json_file, indent=4)\n",
    "\n",
    "#     print(f\"JSON file '{output_filename}' created for training and testing sets.\")\n",
    "\n",
    "# # Example usage\n",
    "# csv_path = \"/local/data1/honzh073/data/hospital18.csv\"\n",
    "# output_folder = \"/local/data1/honzh073/data\"\n",
    "\n",
    "# create_dataset_json(csv_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train集合:\n",
      "患者ID数量: 32\n",
      "AFF标签的图片数量: 30 (34.09%)\n",
      "NFF标签的图片数量: 58 (65.91%)\n",
      "\n",
      "Val集合:\n",
      "患者ID数量: 25\n",
      "AFF标签的图片数量: 13 (34.21%)\n",
      "NFF标签的图片数量: 25 (65.79%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "\n",
    "# def print_statistics(json_file_path, dataset_type):\n",
    "#     # 读取JSON文件\n",
    "#     with open(json_file_path, 'r') as file:\n",
    "#         data = json.load(file)\n",
    "\n",
    "#     # 将JSON数据转换为DataFrame\n",
    "#     df = pd.DataFrame(data[dataset_type])\n",
    "\n",
    "#     # 统计患者ID数量\n",
    "#     num_patients = df['PatientID'].nunique()\n",
    "\n",
    "#     # 统计AFF和NFF标签的图片数量\n",
    "#     num_aff_images = df[df['Label'] == 'AFF'].shape[0]\n",
    "#     num_nff_images = df[df['Label'] == 'NFF'].shape[0]\n",
    "\n",
    "#     # 计算比例\n",
    "#     ratio_aff = num_aff_images / len(df)\n",
    "#     ratio_nff = num_nff_images / len(df)\n",
    "\n",
    "#     # 打印统计信息和比例\n",
    "#     print(f\"{dataset_type.capitalize()}集合:\")\n",
    "#     print(f\"患者ID数量: {num_patients}\")\n",
    "#     print(f\"AFF标签的图片数量: {num_aff_images} ({ratio_aff:.2%})\")\n",
    "#     print(f\"NFF标签的图片数量: {num_nff_images} ({ratio_nff:.2%})\")\n",
    "#     print()\n",
    "\n",
    "# # Example usage\n",
    "# json_file_path = \"/local/data1/honzh073/data/hospital18_2.json\"\n",
    "# print_statistics(json_file_path, 'train')\n",
    "# print_statistics(json_file_path, 'val')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
