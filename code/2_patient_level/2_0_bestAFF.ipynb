{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create image csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# Image path\n",
    "# image_folder = \"/local/data1/honzh073/data/8bit_downsample\"\n",
    "image_folder = \"/local/data1/honzh073/data/8bit_downsample\"\n",
    "\n",
    "# CSV path\n",
    "csv_file_path = \"/local/data1/honzh073/local_repo/FL/code/2_patient_level/image_data.csv\"\n",
    "\n",
    "# Open the CSV file in write mode\n",
    "with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "    # Define the CSV header fields\n",
    "    fieldnames = ['HospitalID', 'PatientID', 'ImageID', 'ImagePath', 'Label']\n",
    "    \n",
    "    # Create a CSV writer object and write the header\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Iterate through files in the folder\n",
    "    for filename in sorted(os.listdir(image_folder)):\n",
    "        # Construct the complete image file path\n",
    "        image_path = os.path.join(image_folder, filename)\n",
    "\n",
    "        # Parse the filename to extract HospitalID, PatientID, image number, and image label\n",
    "        parts = filename.split('_')\n",
    "        hospital_id = parts[3]\n",
    "        patient_id = parts[1]\n",
    "        image_number = parts[6]\n",
    "        image_label = parts[4]\n",
    "\n",
    "        # Write data into the CSV file\n",
    "        writer.writerow({\n",
    "            'HospitalID': hospital_id,\n",
    "            'PatientID': patient_id,\n",
    "            'ImageID': image_number,\n",
    "            'ImagePath': image_path,\n",
    "            'Label': image_label\n",
    "        })\n",
    "\n",
    "# Print a message indicating that the CSV file has been created and saved\n",
    "print(\"csv file:\", csv_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split csv files by patient ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "\n",
    "input_csv_path = \"/local/data1/honzh073/local_repo/FL/code/2_patient_level/image_data.csv\"\n",
    "patient_data = defaultdict(list)\n",
    "\n",
    "with open(input_csv_path, 'r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        patient_id = row['PatientID']\n",
    "        patient_data[patient_id].append(row)\n",
    "\n",
    "num_patients = len(patient_data)\n",
    "\n",
    "# ratio\n",
    "train_ratio = 0.6\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "\n",
    "num_train = int(num_patients * train_ratio)\n",
    "num_val = int(num_patients * val_ratio)\n",
    "num_test = num_patients - num_train - num_val\n",
    "\n",
    "# get random IDs\n",
    "all_patient_ids = list(patient_data.keys())\n",
    "\n",
    "random.seed(1)\n",
    "random.shuffle(all_patient_ids)\n",
    "\n",
    "# split by ID\n",
    "train_patients = all_patient_ids[ : num_train]\n",
    "val_patients = all_patient_ids[num_train : num_train + num_val]\n",
    "test_patients = all_patient_ids[num_train + num_val : ]\n",
    "\n",
    "# iter by patient IDs\n",
    "train_data = []\n",
    "val_data = []\n",
    "test_data = []\n",
    "\n",
    "for patient_id, images in patient_data.items():\n",
    "    if patient_id in train_patients:\n",
    "        train_data.extend(images)\n",
    "    elif patient_id in val_patients:\n",
    "        val_data.extend(images)\n",
    "    elif patient_id in test_patients:\n",
    "        test_data.extend(images)\n",
    "\n",
    "# write to csv\n",
    "def write_to_csv(file_path, data):\n",
    "    with open(file_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=data[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "write_to_csv('/local/data1/honzh073/local_repo/FL/code/2_patient_level/train.csv', train_data)\n",
    "write_to_csv('/local/data1/honzh073/local_repo/FL/code/2_patient_level/val.csv', val_data)\n",
    "write_to_csv('/local/data1/honzh073/local_repo/FL/code/2_patient_level/test.csv', test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# change first layer to single channel image input\n",
    "def first_layer(model):\n",
    "    pretrained_weights = model.conv1.weight\n",
    "    new_weights = torch.nn.Parameter(pretrained_weights[:, 0:1, :, :])\n",
    "    model.conv1.weight = new_weights\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# get training class weight, aff : nff = 2:8\n",
    "def get_classweight(train_dataset):\n",
    "    train_nff_count = sum(1 for _, label in train_dataset if label == 0)  # 0 NFF\n",
    "    train_aff_count = sum(1 for _, label in train_dataset if label == 1)  # 1 AFF\n",
    "    class_weight_nff = 1 / (2 * (train_nff_count / (train_nff_count + train_aff_count)))\n",
    "    class_weight_aff = 1 / (2 * (train_aff_count / (train_nff_count + train_aff_count)))\n",
    "    \n",
    "    return [class_weight_nff, class_weight_aff]   \n",
    "\n",
    "# training function\n",
    "def train_model(train_loader, validation_loader, classweight, num_epochs, lr, step_size, gamma, model_name):\n",
    "    # Load pre-trained model\n",
    "    torch.hub.set_dir('/local/data1/honzh073/download/TORCH_PRETRAINED')\n",
    "    \n",
    "    if model_name == 'resnet152':\n",
    "        from torchvision.models import resnet152, ResNet152_Weights\n",
    "        model = models.resnet152(weights=ResNet152_Weights.DEFAULT)\n",
    "        \n",
    "    elif model_name == 'densenet161':\n",
    "        from torchvision.models import densenet161, DenseNet161_Weights\n",
    "        model = models.densenet161(weights=DenseNet161_Weights.DEFAULT)\n",
    "        \n",
    "        # densenet161 doesnt have conv1\n",
    "        pretrained_weights = model.features.conv0.weight\n",
    "        new_weights = torch.nn.Parameter(pretrained_weights[:, 0:1, :, :])\n",
    "        model.features.conv0.weight = new_weights\n",
    "    \n",
    "    elif model_name == 'resnet50':\n",
    "        from torchvision.models import resnet50, ResNet50_Weights\n",
    "        model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "    elif model_name == 'resnet18':\n",
    "        from torchvision.models import resnet18, ResNet18_Weights\n",
    "        model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "    elif model_name == 'vgg19':\n",
    "        from torchvision.models import vgg19, VGG19_Weights\n",
    "        model = models.vgg19(weights=VGG19_Weights.DEFAULT)\n",
    "\n",
    "    elif model_name == 'resnet101':\n",
    "        from torchvision.models import resnet101, ResNet101_Weights\n",
    "        model = models.resnet101(weights=ResNet101_Weights.DEFAULT)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type. 'vgg19' 'resnet50' 'resnet101' 'resnet152' or 'densenet161'.\")\n",
    "    \n",
    "    # freeze all layers except full connection layer (not a good option)\n",
    "    \n",
    "    # for param in model.parameters():\n",
    "    #     param.requires_grad = False\n",
    "        \n",
    "    num_class = 2 # binary classification\n",
    "    \n",
    "    if model_name == 'densenet161':\n",
    "        in_features = model.classifier.in_features\n",
    "        model.classifier = nn.Sequential(nn.Dropout(0.4),nn.Linear(in_features, num_class))\n",
    "        model.classifier = nn.Sequential(nn.Linear(in_features, num_class))\n",
    "\n",
    "    else:\n",
    "        model = first_layer(model)\n",
    "        in_features = model.fc.in_features\n",
    "        model.fc = nn.Sequential(nn.Linear(in_features, num_class))\n",
    "        model.fc = nn.Sequential(nn.Dropout(0.4), nn.Linear(in_features, num_class))\n",
    "    \n",
    "    # device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # DataParallel speed up?\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"multiple GPU:\", torch.cuda.device_count())\n",
    "        model = nn.DataParallel(model)\n",
    "    else:\n",
    "        print(\"single GPU\")\n",
    "        model = model.to(device)\n",
    "    \n",
    "    # loss\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.Tensor(classweight).to(device))\n",
    "\n",
    "    if model_name == 'densenet161':\n",
    "        # fc parameter\n",
    "        params_fc = model.module.classifier.parameters()  #  'classifier' not 'fc'\n",
    "\n",
    "        # body parameter, except fc\n",
    "        params_1x = [param for name, param in model.module.named_parameters() if 'classifier' not in name]\n",
    "\n",
    "        optimizer = torch.optim.Adam([{'params': params_1x}, {'params': params_fc, 'lr': lr * 10}],\n",
    "                                    lr=lr, weight_decay=5e-4)\n",
    "\n",
    "    else:\n",
    "        # Loss, optimizer\n",
    "        # optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # 获取DataParallel模型的module属性（即实际的模型）\n",
    "        # actual_model = model.module\n",
    "\n",
    "        # 获取全连接层的参数\n",
    "        params_fc = model.module.fc.parameters()\n",
    "\n",
    "        # 获取网络主体的参数（除了全连接层以外的其他层）\n",
    "        params_1x = [param for name, param in model.module.named_parameters() if 'fc' not in name]\n",
    "\n",
    "        # 使用这些参数创建优化器\n",
    "        optimizer = torch.optim.Adam([{'params': params_1x}, {'params': params_fc, 'lr': lr * 10}],\n",
    "                                    lr=lr, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "    # scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "        \n",
    "    # Loss, ACC\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    train_accuracies = []\n",
    "    validation_accuracies = []\n",
    "    \n",
    "    # select best model\n",
    "    best_accuracy = 0.0\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "        \n",
    "        model.eval()\n",
    "        correct_validation_aff = 0\n",
    "        total_validation_aff = 0\n",
    "        correct_validation = 0\n",
    "        total_validation = 0\n",
    "        validation_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in validation_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_validation += labels.size(0)\n",
    "                correct_validation += (predicted == labels).sum().item()\n",
    "                validation_loss += loss.item()\n",
    "                # Calculate accuracy for AFF class\n",
    "                total_validation_aff += torch.sum(labels == 1).item()\n",
    "                correct_validation_aff += torch.sum((predicted == 1) & (labels == 1)).item()\n",
    "\n",
    "        validation_accuracy_aff = 100 * correct_validation_aff / total_validation_aff\n",
    "\n",
    "        # validation accuracy and loss\n",
    "        validation_accuracy = 100 * correct_validation / total_validation\n",
    "        validation_loss /= len(validation_loader)\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        validation_losses.append(validation_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        validation_accuracies.append(validation_accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "            f\"train Loss: {loss.item():.4f}, \"\n",
    "            f\"val Loss: {validation_loss:.4f}, \"\n",
    "            f\"train ACC: {train_accuracy:.2f}%, \"\n",
    "            f\"Val ACC: {validation_accuracy:.2f}%\")\n",
    "        \n",
    "        # scheduler.step()  # Step the learning rate scheduler\n",
    "                # Update best model if current accuracy is higher\n",
    "        \n",
    "        # select by aff\n",
    "        if validation_accuracy_aff > best_accuracy:\n",
    "            best_accuracy = validation_accuracy_aff\n",
    "            best_model = model.module if isinstance(model, nn.DataParallel) else model\n",
    "\n",
    "        # if validation_accuracy > best_validation_accuracy:\n",
    "        #     best_validation_accuracy = validation_accuracy\n",
    "        #     best_model = model.module  # DataParallel\n",
    "        #     # best_model = model  # single device\n",
    "            \n",
    "    # Plot train/val loss,  accuracy\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(validation_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(validation_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend() \n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def test_model(model, test_dataset, batch_size):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    test_loss = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    test_accuracy = 100 * correct_test / total_test\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.2f}%\")\n",
    "     \n",
    "    auc_score = roc_auc_score(all_labels, all_predictions)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    # Precision、Recall、F1 Score\n",
    "    class_labels = {0: 'NFF', 1: 'AFF'}  # Define your class labels here\n",
    "\n",
    "    classification_rep = classification_report(all_labels, all_predictions, target_names=[class_labels[i] for i in range(len(class_labels))])\n",
    "    print(\"AUC:\", auc_score)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_rep)\n",
    "\n",
    "    plot_roc_curve(all_labels, all_predictions)\n",
    "    \n",
    "def plot_roc_curve(all_labels, all_predictions):\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_predictions)\n",
    "    roc_auc = roc_auc_score(all_labels, all_predictions)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "def show_image(dataset, num_images=5):\n",
    "    # Get random indices\n",
    "    random_indices = np.random.choice(len(dataset), num_images, replace=False)\n",
    "\n",
    "    # Plot images with truncated names\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, idx in enumerate(random_indices):\n",
    "        image, label = dataset[idx]  # Use the dataset directly\n",
    "        filename = dataset.data[idx][0]  # Get the filename from dataset's internal data attribute\n",
    "        truncated_filename = filename.split('/')[-1][:15]  # Extract the last part and truncate to 15 characters\n",
    "        \n",
    "        # Print the original filename\n",
    "        print(f\"Image location: {filename}\")\n",
    "\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.title(f\"Label: {label}\")\n",
    "        # plt.imshow(image[0])  # Assuming single-channel (grayscale) image\n",
    "        plt.imshow(image[0], cmap='gray')  # Assuming single-channel (grayscale) image\n",
    "\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import csv\n",
    "\n",
    "# read image, image by csv. Custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = []\n",
    "        self.transform = transform\n",
    "        \n",
    "        # read csv\n",
    "        with open(csv_file, 'r') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                image_path = row['ImagePath']\n",
    "                label = row['Label']\n",
    "\n",
    "                if label == 'NFF':\n",
    "                    label = 0\n",
    "                elif label == 'AFF':\n",
    "                    label = 1\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid label in CSV file.\")\n",
    "                self.data.append((image_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.data[idx]\n",
    "        image = Image.open(image_path).convert('L')  # 'L' to gray image\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Define data augmentation transforms for training data\n",
    "\n",
    "# train_transform = transforms.Compose([\n",
    "#     transforms.RandomHorizontalFlip(p=1),\n",
    "#     transforms.RandomVerticalFlip(p=1),\n",
    "#     transforms.RandomRotation(degrees=(0, 180)),\n",
    "#     transforms.ColorJitter(brightness=0.5, contrast=0.1, saturation=0.1, hue=0.3),\n",
    "#     # transforms.RandomApply([transforms.RandomAffine(degrees=0, translate=(0.1, 0.1))], p=1),\n",
    "#     # transforms.GaussianBlur(kernel_size=(5,9), sigma=(0.1, 5)),\n",
    "#     # transforms.RandomInvert(),\n",
    "#     # transforms.RandomPosterize(bits=2),\n",
    "#     transforms.RandomAdjustSharpness(sharpness_factor=4),\n",
    "#     # transforms.RandomAutocontrast(),\n",
    "#     # transforms.RandomApply([transforms.RandomPerspective(distortion_scale=0.2)], p=1),\n",
    "#     # transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "#     transforms.ToTensor(),\n",
    "#     # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#     transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "\n",
    "# ])\n",
    "\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.RandomApply([transforms.RandomAffine(degrees=0, translate=(0.1, 0.1))], p=.5),\n",
    "    transforms.RandomApply([transforms.RandomPerspective(distortion_scale=0.2)], p=.5),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomAdjustSharpness(sharpness_factor=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]) # gray iamge 0 to 1\n",
    "])\n",
    "\n",
    "# only use     transforms.RandomHorizontalFlip(p=0.5), transforms.RandomVerticalFlip(p=0.5),    transforms.RandomAdjustSharpness(sharpness_factor=4),\n",
    "# which has a better performance\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]) # add mean and std avoid randomness\n",
    "\n",
    "])\n",
    "\n",
    "train_dataset = CustomDataset('/local/data1/honzh073/local_repo/FL/code/3_single_hospital/csv_files/train.csv', transform=train_transform)\n",
    "val_dataset = CustomDataset('/local/data1/honzh073/local_repo/FL/code/3_single_hospital/csv_files/val.csv', transform=test_transform)\n",
    "test_dataset = CustomDataset('/local/data1/honzh073/local_repo/FL/code/3_single_hospital/csv_files/test.csv', transform=test_transform)\n",
    "\n",
    "# repeated_dataset = torch.utils.data.ConcatDataset([train_dataset] * 30)\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# classweight\n",
    "classweight = get_classweight(train_dataset)\n",
    "print('training class weight', classweight)\n",
    "\n",
    "# device\n",
    "# device = torch.device(\"cuda:0\" if  torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "show_image(train_dataset, 10)\n",
    "\n",
    "print(\"Saved train.csv, val.csv, test.csv.\")\n",
    "print('-----------------------------------')\n",
    "AFF_label = 'AFF'\n",
    "NFF_label = 'NFF'\n",
    "\n",
    "# propotion of aff and nff in datasets\n",
    "train_AFF_count = sum(1 for row in train_data if row['Label'] == AFF_label)\n",
    "train_NFF_count = sum(1 for row in train_data if row['Label'] == NFF_label)\n",
    "\n",
    "val_AFF_count = sum(1 for row in val_data if row['Label'] == AFF_label)\n",
    "val_NFF_count = sum(1 for row in val_data if row['Label'] == NFF_label)\n",
    "\n",
    "test_AFF_count = sum(1 for row in test_data if row['Label'] == AFF_label)\n",
    "test_NFF_count = sum(1 for row in test_data if row['Label'] == NFF_label)\n",
    "\n",
    "print(f\"train AFF: {train_AFF_count}, ratio: {train_AFF_count / (train_AFF_count + train_NFF_count):.2f}\")\n",
    "print(f\"----- NFF: {train_NFF_count}, ratio: {train_NFF_count / (train_AFF_count + train_NFF_count):.2f}\")\n",
    "print('-----------------------------------')\n",
    "print(f\"val AFF: {val_AFF_count}, ratio: {val_AFF_count / (val_AFF_count + val_NFF_count):.2f}\")\n",
    "print(f\"--- NFF: {val_NFF_count}, ratio: {val_NFF_count / (val_AFF_count + val_NFF_count):.2f}\")\n",
    "print('-----------------------------------')\n",
    "print(f\"test AFF: {test_AFF_count}, ratio: {test_AFF_count / (test_AFF_count + test_NFF_count):.2f}\")\n",
    "print(f\"---- NFF: {test_NFF_count}, ratio: {test_NFF_count / (test_AFF_count + test_NFF_count):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = 50\n",
    "# resenet 18\n",
    "resnet18 = train_model(train_loader, val_loader,\n",
    "                       classweight=classweight,\n",
    "                       num_epochs=epoch_num,\n",
    "                       lr=0.0001, step_size=10, gamma=0.1,\n",
    "                       model_name='resnet18')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ResNet 18')\n",
    "test_model(model=resnet18, test_dataset=test_dataset, batch_size=batch_size)\n",
    "\n",
    "params_count = count_parameters(resnet18)\n",
    "print(f\"number of parameters: {params_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resnet50 = train_model(train_loader, val_loader,\n",
    "                       classweight=classweight,\n",
    "                       num_epochs=epoch_num,\n",
    "                       lr=0.0001, step_size=10, gamma=0.1,\n",
    "                       model_name='resnet50')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ResNet 50')\n",
    "test_model(model=resnet50, test_dataset=test_dataset, batch_size=batch_size)\n",
    "params_count = count_parameters(resnet50)\n",
    "print(f\"number of parameters: {params_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet101\n",
    "resnet101 = train_model(train_loader, val_loader, classweight, num_epochs=epoch_num, lr=0.0001, step_size=7, gamma=0.1, model_name='resnet101')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ResNet 101')\n",
    "test_model(model=resnet101, test_dataset=test_dataset, batch_size=batch_size)\n",
    "params_count = count_parameters(resnet101)\n",
    "print(f\"number of parameters: {params_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet152\n",
    "resnet152 = train_model(train_loader, val_loader, classweight, \n",
    "                        num_epochs=epoch_num, lr=0.0001, step_size=10, gamma=0.1, model_name='resnet152')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ResNet 152')\n",
    "test_model(model=resnet152, test_dataset=test_dataset, batch_size=batch_size)\n",
    "params_count = count_parameters(resnet152)\n",
    "print(f\"number of parameters: {params_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# densenet161\n",
    "densenet161 = train_model(train_loader, val_loader, classweight, \n",
    "                          num_epochs=50, lr=0.0001, step_size=10, gamma=0.1, model_name='densenet161')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model=densenet161, test_dataset=test_dataset, batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
