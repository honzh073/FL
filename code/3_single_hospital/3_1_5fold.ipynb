{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# Image path\n",
    "image_folder = \"/local/data1/honzh073/data/8bit_downsample\"\n",
    "\n",
    "# CSV path\n",
    "csv_file_path = \"/local/data1/honzh073/local_repo/FL/code/3_single_hospital/csv_files/image_data.csv\"\n",
    "\n",
    "# Open the CSV file in write mode\n",
    "with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "    # Define the CSV header fields\n",
    "    fieldnames = ['HospitalID', 'PatientID', 'ImageID', 'ImagePath', 'Label']\n",
    "    \n",
    "    # Create a CSV writer object and write the header\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Iterate through files in the folder\n",
    "    for filename in sorted(os.listdir(image_folder)):\n",
    "        # Construct the complete image file path\n",
    "        image_path = os.path.join(image_folder, filename)\n",
    "\n",
    "        # Parse the filename to extract HospitalID, PatientID, image number, and image label\n",
    "        parts = filename.split('_')\n",
    "        hospital_id = parts[3]\n",
    "        patient_id = parts[1]\n",
    "        image_number = parts[6]\n",
    "        image_label = parts[4]\n",
    "\n",
    "        # Write data into the CSV file\n",
    "        writer.writerow({\n",
    "            'HospitalID': hospital_id,\n",
    "            'PatientID': patient_id,\n",
    "            'ImageID': image_number,\n",
    "            'ImagePath': image_path,\n",
    "            'Label': image_label\n",
    "        })\n",
    "\n",
    "# Print a message indicating that the CSV file has been created and saved\n",
    "print(\"CSV file has been created and saved to:\", csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# 指定特定的医院ID\n",
    "target_hospital_id = '43'  # 替换成你想要选择的医院ID\n",
    "\n",
    "# 读取原始CSV文件并筛选特定医院ID的数据\n",
    "input_csv_path = \"/local/data1/honzh073/local_repo/FL/code/3_single_hospital/csv_files/image_data.csv\"\n",
    "patient_data = defaultdict(list)\n",
    "\n",
    "with open(input_csv_path, 'r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        if row['HospitalID'] == target_hospital_id:\n",
    "            patient_id = row['PatientID']\n",
    "            patient_data[patient_id].append(row)\n",
    "\n",
    "# 计算患者ID的数量\n",
    "num_patients = len(patient_data)\n",
    "\n",
    "# 计算划分的数量\n",
    "num_train = int(num_patients * 0.7)\n",
    "num_test = num_patients - num_train\n",
    "\n",
    "# 获取随机选择的患者ID\n",
    "all_patient_ids = list(patient_data.keys())\n",
    "random.seed(123)\n",
    "random.shuffle(all_patient_ids)\n",
    "\n",
    "# 划分数据集\n",
    "train_patients = all_patient_ids[:num_train]\n",
    "test_patients = all_patient_ids[num_train:]\n",
    "\n",
    "# 用于存储划分后的数据\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "# 遍历按照患者ID分组的数据，并将数据划分到对应的数据集中\n",
    "for patient_id, images in patient_data.items():\n",
    "    if patient_id in train_patients:\n",
    "        train_data.extend(images)\n",
    "    elif patient_id in test_patients:\n",
    "        test_data.extend(images)\n",
    "\n",
    "# 将数据集写入CSV文件\n",
    "def write_to_csv(file_path, data):\n",
    "    with open(file_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=data[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "write_to_csv('/local/data1/honzh073/local_repo/FL/code/3_single_hospital/csv_files/k_fold_train.csv', train_data)\n",
    "write_to_csv('/local/data1/honzh073/local_repo/FL/code/3_single_hospital/csv_files/k_fold_test.csv', test_data)\n",
    "\n",
    "print(\"训练数据、验证数据和测试数据已经生成并保存到 train.csv, val.csv 和 test.csv。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import csv\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "def get_classweight(train_dataset):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # aff and nff numbers in training dataset\n",
    "    train_nff_count = sum(1 for _, label in train_dataset if label == 0)  # 0 NFF\n",
    "    train_aff_count = sum(1 for _, label in train_dataset if label == 1)  # 1 AFF\n",
    "\n",
    "    class_weight_nff = 1 / (2 * (train_nff_count / (train_nff_count + train_aff_count)))\n",
    "    class_weight_aff = 1 / (2 * (train_aff_count / (train_nff_count + train_aff_count)))\n",
    "    \n",
    "    return [class_weight_nff, class_weight_aff]   \n",
    "    \n",
    "def train_model(train_loader, validation_loader, classweight, num_epochs, lr, step_size, gamma, model_name, device):\n",
    "\n",
    "    # Load pre-trained model\n",
    "    torch.hub.set_dir('/local/data1/honzh073/download/TORCH_PRETRAINED')\n",
    "    if model_name == 'resnet152':\n",
    "        from torchvision.models import resnet152, ResNet152_Weights\n",
    "        model = models.resnet152(weights=ResNet152_Weights.DEFAULT)\n",
    "        \n",
    "    elif model_name == 'densenet161':\n",
    "        from torchvision.models import densenet161, DenseNet161_Weights\n",
    "        model = models.densenet161(weights=DenseNet161_Weights.DEFAULT)\n",
    "        \n",
    "    elif model_name == 'resnet50':\n",
    "        from torchvision.models import resnet50, ResNet50_Weights\n",
    "        model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "    elif model_name == 'vgg19':\n",
    "        from torchvision.models import vgg19, VGG19_Weights\n",
    "        model = models.vgg19(weights=VGG19_Weights.DEFAULT)\n",
    "\n",
    "    elif model_name == 'resnet101':\n",
    "        from torchvision.models import resnet101, ResNet101_Weights\n",
    "        model = models.resnet101(weights=ResNet101_Weights.DEFAULT)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type. 'vgg19' 'resnet50' 'resnet101' 'resnet152' or 'densenet161'.\")\n",
    "    \n",
    "    # Freeze layers except the last\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.layer4.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    num_classes = 2\n",
    "    if model_name == 'densenet161':\n",
    "        in_features = model.classifier.in_features\n",
    "        # model.classifier = nn.Sequential(nn.Dropout(0.5),nn.Linear(in_features, num_classes))\n",
    "        model.classifier = nn.Sequential(nn.Linear(in_features, num_classes))\n",
    "\n",
    "    else:\n",
    "        in_features = model.fc.in_features\n",
    "        model.fc = nn.Sequential(nn.Linear(in_features, num_classes))\n",
    "        # model.fc = nn.Sequential(nn.Dropout(0.5), nn.Linear(in_features, num_classes))\n",
    "    # Move model to the specified device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.Tensor(classweight).to(device))\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    \n",
    "    # Training (loss and accuracy)\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    train_accuracies = []\n",
    "    validation_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "        \n",
    "        model.eval()\n",
    "        correct_validation = 0\n",
    "        total_validation = 0\n",
    "        validation_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in validation_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_validation += labels.size(0)\n",
    "                correct_validation += (predicted == labels).sum().item()\n",
    "                validation_loss += loss.item()\n",
    "        \n",
    "        # validation accuracy and loss\n",
    "        validation_accuracy = 100 * correct_validation / total_validation\n",
    "        validation_loss /= len(validation_loader)\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        validation_losses.append(validation_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        validation_accuracies.append(validation_accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "            f\"Train Loss: {loss.item():.4f}, \"\n",
    "            f\"Validation Loss: {validation_loss:.4f}, \"\n",
    "            f\"Train Acc: {train_accuracy:.2f}%, \"\n",
    "            f\"Validation Acc: {validation_accuracy:.2f}%\")\n",
    "        \n",
    "        scheduler.step()  # Step the learning rate scheduler\n",
    "\n",
    "    # Plot loss and accuracy\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(validation_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(validation_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend() \n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "def test_model(model, test_dataset, batch_size, device):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    model.eval()\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    test_loss = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    test_accuracy = 100 * correct_test / total_test\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.2f}%\")\n",
    "\n",
    "    # confusion matrix\n",
    "    with torch.no_grad():\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    auc_score = roc_auc_score(all_labels, all_predictions)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    # Precision、Recall、F1 Score\n",
    "    class_labels = {0: 'NFF', 1: 'AFF'}  # Define your class labels here\n",
    "\n",
    "    # Then, when you create the confusion matrix and classification report, use these labels:\n",
    "    classification_rep = classification_report(all_labels, all_predictions, target_names=[class_labels[i] for i in range(len(class_labels))])\n",
    "    print(\"AUC:\", auc_score)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_rep)\n",
    "\n",
    "    plot_roc_curve(all_labels, all_predictions)\n",
    "    \n",
    "def plot_roc_curve(all_labels, all_predictions):\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_predictions)\n",
    "    roc_auc = roc_auc_score(all_labels, all_predictions)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import csv\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "def get_classweight(train_dataset):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # aff and nff numbers in training dataset\n",
    "    train_nff_count = sum(1 for _, label in train_dataset if label == 0)  # 0 NFF\n",
    "    train_aff_count = sum(1 for _, label in train_dataset if label == 1)  # 1 AFF\n",
    "\n",
    "    class_weight_nff = 1 / (2 * (train_nff_count / (train_nff_count + train_aff_count)))\n",
    "    class_weight_aff = 1 / (2 * (train_aff_count / (train_nff_count + train_aff_count)))\n",
    "    \n",
    "    return [class_weight_nff, class_weight_aff]   \n",
    "    \n",
    "def train_model(train_loader, validation_loader, classweight, num_epochs, lr, step_size, gamma, model_name, device):\n",
    "\n",
    "    # Load pre-trained model\n",
    "    torch.hub.set_dir('/local/data1/honzh073/download/TORCH_PRETRAINED')\n",
    "    if model_name == 'resnet152':\n",
    "        from torchvision.models import resnet152, ResNet152_Weights\n",
    "        model = models.resnet152(weights=ResNet152_Weights.DEFAULT)\n",
    "        \n",
    "    elif model_name == 'densenet161':\n",
    "        from torchvision.models import densenet161, DenseNet161_Weights\n",
    "        model = models.densenet161(weights=DenseNet161_Weights.DEFAULT)\n",
    "        \n",
    "    elif model_name == 'resnet50':\n",
    "        from torchvision.models import resnet50, ResNet50_Weights\n",
    "        model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "    elif model_name == 'vgg19':\n",
    "        from torchvision.models import vgg19, VGG19_Weights\n",
    "        model = models.vgg19(weights=VGG19_Weights.DEFAULT)\n",
    "\n",
    "    elif model_name == 'resnet101':\n",
    "        from torchvision.models import resnet101, ResNet101_Weights\n",
    "        model = models.resnet101(weights=ResNet101_Weights.DEFAULT)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type. 'vgg19' 'resnet50' 'resnet101' 'resnet152' or 'densenet161'.\")\n",
    "    \n",
    "    # Freeze layers except the last\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.layer4.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    num_classes = 2\n",
    "    if model_name == 'densenet161':\n",
    "        in_features = model.classifier.in_features\n",
    "        # model.classifier = nn.Sequential(nn.Dropout(0.5),nn.Linear(in_features, num_classes))\n",
    "        model.classifier = nn.Sequential(nn.Linear(in_features, num_classes))\n",
    "\n",
    "    else:\n",
    "        in_features = model.fc.in_features\n",
    "        model.fc = nn.Sequential(nn.Linear(in_features, num_classes))\n",
    "        # model.fc = nn.Sequential(nn.Dropout(0.5), nn.Linear(in_features, num_classes))\n",
    "    # Move model to the specified device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.Tensor(classweight).to(device))\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    \n",
    "    # Training (loss and accuracy)\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    train_accuracies = []\n",
    "    validation_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "        \n",
    "        model.eval()\n",
    "        correct_validation = 0\n",
    "        total_validation = 0\n",
    "        validation_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in validation_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_validation += labels.size(0)\n",
    "                correct_validation += (predicted == labels).sum().item()\n",
    "                validation_loss += loss.item()\n",
    "        \n",
    "        # validation accuracy and loss\n",
    "        validation_accuracy = 100 * correct_validation / total_validation\n",
    "        validation_loss /= len(validation_loader)\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        validation_losses.append(validation_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        validation_accuracies.append(validation_accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "            f\"Train Loss: {loss.item():.4f}, \"\n",
    "            f\"Validation Loss: {validation_loss:.4f}, \"\n",
    "            f\"Train Acc: {train_accuracy:.2f}%, \"\n",
    "            f\"Validation Acc: {validation_accuracy:.2f}%\")\n",
    "        \n",
    "        scheduler.step()  # Step the learning rate scheduler\n",
    "\n",
    "    # Plot loss and accuracy\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(validation_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(validation_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend() \n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "def test_model(model, test_dataset, batch_size, device):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    model.eval()\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    test_loss = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    test_accuracy = 100 * correct_test / total_test\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.2f}%\")\n",
    "\n",
    "    # confusion matrix\n",
    "    with torch.no_grad():\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    auc_score = roc_auc_score(all_labels, all_predictions)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    # Precision、Recall、F1 Score\n",
    "    class_labels = {0: 'NFF', 1: 'AFF'}  # Define your class labels here\n",
    "\n",
    "    # Then, when you create the confusion matrix and classification report, use these labels:\n",
    "    classification_rep = classification_report(all_labels, all_predictions, target_names=[class_labels[i] for i in range(len(class_labels))])\n",
    "    print(\"AUC:\", auc_score)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_rep)\n",
    "\n",
    "    plot_roc_curve(all_labels, all_predictions)\n",
    "    \n",
    "def plot_roc_curve(all_labels, all_predictions):\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_predictions)\n",
    "    roc_auc = roc_auc_score(all_labels, all_predictions)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models import resnet50\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch\n",
    "from PIL import Image\n",
    "import csv\n",
    "\n",
    "# Define data augmentation transforms for training data\n",
    "# Define data augmentation transforms for training data\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=1),\n",
    "    transforms.RandomVerticalFlip(p=1),\n",
    "    transforms.RandomRotation(degrees=(20)),\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.1, saturation=0.1, hue=0.3),\n",
    "    transforms.RandomApply([transforms.RandomAffine(degrees=0, translate=(0.1, 0.1))], p=1),\n",
    "    transforms.GaussianBlur(kernel_size=(5,9), sigma=(0.1, 5)),\n",
    "    # transforms.RandomInvert(),\n",
    "    # transforms.RandomPosterize(bits=2),\n",
    "    transforms.RandomAdjustSharpness(sharpness_factor=2),\n",
    "    transforms.RandomAutocontrast(),\n",
    "    # transforms.RandomApply([transforms.RandomPerspective(distortion_scale=0.2)], p=1),\n",
    "    # transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = []\n",
    "        self.transform = transform\n",
    "        \n",
    "        # 读取CSV文件\n",
    "        with open(csv_file, 'r') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                image_path = row['ImagePath']\n",
    "                label = row['Label']\n",
    "                # 如果Label是‘NFF’，定义为0；如果label是‘AFF’，定义为1\n",
    "                if label == 'NFF':\n",
    "                    label = 0\n",
    "                elif label == 'AFF':\n",
    "                    label = 1\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid label in CSV file.\")\n",
    "                self.data.append((image_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.data[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# 使用StratifiedKFold将数据分成5个fold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 读取训练数据的CSV文件路径\n",
    "train_csv_path = '/local/data1/honzh073/local_repo/FL/code/3_single_hospital/csv_files/k_fold_train.csv'\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 将数据分成5个fold，每个fold进行训练和验证\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(range(len(train_data)), train_data['Label'])):\n",
    "    # 创建训练集和验证集的数据加载器\n",
    "    train_dataset = CustomDataset(train_csv_path, transform=train_transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    val_dataset = CustomDataset(train_csv_path, transform=test_transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "    \n",
    "    classweight = get_classweight(train_dataset)\n",
    "\n",
    "    resnet101 = train_model(train_loader, val_loader,\n",
    "                        classweight=classweight,\n",
    "                        num_epochs=50,\n",
    "                        lr=0.0001, step_size=25, gamma=0.1,\n",
    "                        device=device,\n",
    "                        model_name='resnet101')\n",
    "\n",
    "# 最后，在测试集上进行测试\n",
    "test_dataset = CustomDataset('/local/data1/honzh073/local_repo/FL/code/3_single_hospital/csv_files/k_fold_test.csv', transform=test_transform)\n",
    "test_model(model=resnet101, test_dataset=test_dataset, batch_size=64, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/data1/honzh073/anaconda3/envs/myenv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/local/data1/honzh073/anaconda3/envs/myenv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'DataFrame'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/local/data1/honzh073/local_repo/FL/code/3_single_hospital/3_1_5fold.ipynb Cell 7\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repo/FL/code/3_single_hospital/3_1_5fold.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m kf \u001b[39m=\u001b[39m KFold(n_splits\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repo/FL/code/3_single_hospital/3_1_5fold.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39mfor\u001b[39;00m fold, (train_idx, val_idx) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(kf\u001b[39m.\u001b[39msplit(train_data)):  \u001b[39m# 使用train_data而不是range(len(train_csv_path))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repo/FL/code/3_single_hospital/3_1_5fold.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m     \u001b[39m# 创建训练集和验证集的数据加载器\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repo/FL/code/3_single_hospital/3_1_5fold.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m     train_dataset \u001b[39m=\u001b[39m CustomDataset(train_data\u001b[39m.\u001b[39;49miloc[train_idx], transform\u001b[39m=\u001b[39;49mtrain_transform)  \u001b[39m# 使用train_data的子集\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repo/FL/code/3_single_hospital/3_1_5fold.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m     train_loader \u001b[39m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repo/FL/code/3_single_hospital/3_1_5fold.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m     val_dataset \u001b[39m=\u001b[39m CustomDataset(train_data\u001b[39m.\u001b[39miloc[val_idx], transform\u001b[39m=\u001b[39mtest_transform)  \u001b[39m# 使用train_data的子集\u001b[39;00m\n",
      "\u001b[1;32m/local/data1/honzh073/local_repo/FL/code/3_single_hospital/3_1_5fold.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repo/FL/code/3_single_hospital/3_1_5fold.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39m=\u001b[39m transform\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repo/FL/code/3_single_hospital/3_1_5fold.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# 读取CSV文件\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repo/FL/code/3_single_hospital/3_1_5fold.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(csv_file, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m csvfile:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repo/FL/code/3_single_hospital/3_1_5fold.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m     reader \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mDictReader(csvfile)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repo/FL/code/3_single_hospital/3_1_5fold.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m reader:\n",
      "File \u001b[0;32m/local/data1/honzh073/anaconda3/envs/myenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:277\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(io_open)\n\u001b[1;32m    276\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_modified_open\u001b[39m(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 277\u001b[0m     \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;49;00m {\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m}:\n\u001b[1;32m    278\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         )\n\u001b[1;32m    284\u001b[0m     \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'DataFrame'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models import resnet101\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch\n",
    "from PIL import Image\n",
    "import csv\n",
    "\n",
    "# 定义数据增强的transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=20),\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.1, saturation=0.1, hue=0.3),\n",
    "    transforms.RandomApply([transforms.RandomAffine(degrees=0, translate=(0.1, 0.1))], p=0.5),\n",
    "    transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "    transforms.RandomAdjustSharpness(sharpness_factor=2),\n",
    "    transforms.RandomAutocontrast(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# 定义自定义数据集类\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = []\n",
    "        self.transform = transform\n",
    "        \n",
    "        # 读取CSV文件\n",
    "        with open(csv_file, 'r') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                image_path = row['ImagePath']\n",
    "                label = int(row['Label'] == 'AFF')  # 将AFF定义为1，其他定义为0\n",
    "                self.data.append((image_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.data[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# 读取训练数据的CSV文件路径\n",
    "train_csv_path = '/local/data1/honzh073/local_repo/FL/code/3_single_hospital/csv_files/k_fold_train.csv'\n",
    "train_data = pd.read_csv(train_csv_path)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 创建ResNet-101模型和优化器\n",
    "model = resnet101(pretrained=True)\n",
    "num_classes = 2  # 二分类任务\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# 定义损失函数、优化器和学习率调度器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.1)\n",
    "\n",
    "# 使用KFold将数据分成5个fold，根据患者ID划分\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# 使用KFold将数据分成5个fold，根据患者ID划分\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_data)):  # 使用train_data而不是range(len(train_csv_path))\n",
    "    # 创建训练集和验证集的数据加载器\n",
    "    train_dataset = CustomDataset(train_data.iloc[train_idx], transform=train_transform)  # 使用train_data的子集\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    val_dataset = CustomDataset(train_data.iloc[val_idx], transform=test_transform)  # 使用train_data的子集\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "    \n",
    "    resnet101 = train_model(train_loader, val_loader,\n",
    "                        classweight=classweight,\n",
    "                        num_epochs=50,\n",
    "                        lr=0.0001, step_size=25, gamma=0.1,\n",
    "                        device=device,\n",
    "                        model_name='resnet101')\n",
    "\n",
    "# 最后，在测试集上进行测试\n",
    "test_csv_path = '/local/data1/honzh073/local_repo/FL/code/3_single_hospital/csv_files/k_fold_test.csv'\n",
    "test_dataset = CustomDataset(test_csv_path, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
