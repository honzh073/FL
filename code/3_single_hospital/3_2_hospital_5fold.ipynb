{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: '/local/data1/honzh073/data/hospital_43_5fold/train/AFF/patient_RCQOXVJTTC'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/local/data1/honzh073/git/FL/localnn/3_single_hospital/3_2_hospital_5fold.ipynb Cell 1\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/git/FL/localnn/3_single_hospital/3_2_hospital_5fold.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     source_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(source_folder, patient_id)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/git/FL/localnn/3_single_hospital/3_2_hospital_5fold.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m     target_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(target_dataset_folder, \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, fracture_type, patient_id)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/git/FL/localnn/3_single_hospital/3_2_hospital_5fold.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m     shutil\u001b[39m.\u001b[39;49mcopytree(source_path, target_path)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/git/FL/localnn/3_single_hospital/3_2_hospital_5fold.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# for patient_id in val_patients:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/git/FL/localnn/3_single_hospital/3_2_hospital_5fold.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m#     source_path = os.path.join(source_folder, patient_id)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/git/FL/localnn/3_single_hospital/3_2_hospital_5fold.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m#     target_path = os.path.join(target_dataset_folder, 'val', fracture_type, patient_id)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/git/FL/localnn/3_single_hospital/3_2_hospital_5fold.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m#     shutil.copytree(source_path, target_path)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/git/FL/localnn/3_single_hospital/3_2_hospital_5fold.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mfor\u001b[39;00m patient_id \u001b[39min\u001b[39;00m test_patients:\n",
      "File \u001b[0;32m/local/data1/honzh073/anaconda3/envs/myenv/lib/python3.9/shutil.py:568\u001b[0m, in \u001b[0;36mcopytree\u001b[0;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[39mwith\u001b[39;00m os\u001b[39m.\u001b[39mscandir(src) \u001b[39mas\u001b[39;00m itr:\n\u001b[1;32m    567\u001b[0m     entries \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(itr)\n\u001b[0;32m--> 568\u001b[0m \u001b[39mreturn\u001b[39;00m _copytree(entries\u001b[39m=\u001b[39;49mentries, src\u001b[39m=\u001b[39;49msrc, dst\u001b[39m=\u001b[39;49mdst, symlinks\u001b[39m=\u001b[39;49msymlinks,\n\u001b[1;32m    569\u001b[0m                  ignore\u001b[39m=\u001b[39;49mignore, copy_function\u001b[39m=\u001b[39;49mcopy_function,\n\u001b[1;32m    570\u001b[0m                  ignore_dangling_symlinks\u001b[39m=\u001b[39;49mignore_dangling_symlinks,\n\u001b[1;32m    571\u001b[0m                  dirs_exist_ok\u001b[39m=\u001b[39;49mdirs_exist_ok)\n",
      "File \u001b[0;32m/local/data1/honzh073/anaconda3/envs/myenv/lib/python3.9/shutil.py:467\u001b[0m, in \u001b[0;36m_copytree\u001b[0;34m(entries, src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    465\u001b[0m     ignored_names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[0;32m--> 467\u001b[0m os\u001b[39m.\u001b[39;49mmakedirs(dst, exist_ok\u001b[39m=\u001b[39;49mdirs_exist_ok)\n\u001b[1;32m    468\u001b[0m errors \u001b[39m=\u001b[39m []\n\u001b[1;32m    469\u001b[0m use_srcentry \u001b[39m=\u001b[39m copy_function \u001b[39mis\u001b[39;00m copy2 \u001b[39mor\u001b[39;00m copy_function \u001b[39mis\u001b[39;00m copy\n",
      "File \u001b[0;32m/local/data1/honzh073/anaconda3/envs/myenv/lib/python3.9/os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     mkdir(name, mode)\n\u001b[1;32m    226\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[39m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[39m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m exist_ok \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m path\u001b[39m.\u001b[39misdir(name):\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/local/data1/honzh073/data/hospital_43_5fold/train/AFF/patient_RCQOXVJTTC'"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import random\n",
    "# import shutil\n",
    "\n",
    "# # 原始数据集根文件夹\n",
    "# dataset_root = '/local/data1/honzh073/data/hospital_43'\n",
    "\n",
    "# # 目标数据集文件夹\n",
    "# target_dataset_folder = '/local/data1/honzh073/data/hospital_43_5fold'\n",
    "\n",
    "# # 创建目标数据集文件夹\n",
    "# os.makedirs(target_dataset_folder, exist_ok=True)\n",
    "\n",
    "# # AFF和NFF文件夹\n",
    "# fracture_types = ['AFF', 'NFF']\n",
    "\n",
    "# # 分割数据集的比例\n",
    "# train_ratio = 0.7\n",
    "# test_ratio = 0.3\n",
    "\n",
    "# # 遍历AFF和NFF文件夹\n",
    "# for fracture_type in fracture_types:\n",
    "#     source_folder = os.path.join(dataset_root, fracture_type)\n",
    "#     patients = os.listdir(source_folder)\n",
    "    \n",
    "#     # 随机打乱患者顺序\n",
    "#     random.shuffle(patients)\n",
    "    \n",
    "#     # 划分数据集\n",
    "#     total_patients = len(patients)\n",
    "#     train_end = int(total_patients * train_ratio)\n",
    "    \n",
    "#     train_patients = patients[:train_end]\n",
    "#     # val_patients = patients[train_end:val_end]\n",
    "#     test_patients = patients[train_end:]\n",
    "    \n",
    "#     # 复制图片到相应的数据集文件夹\n",
    "#     for patient_id in train_patients:\n",
    "#         source_path = os.path.join(source_folder, patient_id)\n",
    "#         target_path = os.path.join(target_dataset_folder, 'train', fracture_type, patient_id)\n",
    "#         shutil.copytree(source_path, target_path)\n",
    "    \n",
    "#     # for patient_id in val_patients:\n",
    "#     #     source_path = os.path.join(source_folder, patient_id)\n",
    "#     #     target_path = os.path.join(target_dataset_folder, 'val', fracture_type, patient_id)\n",
    "#     #     shutil.copytree(source_path, target_path)\n",
    "    \n",
    "#     for patient_id in test_patients:\n",
    "#         source_path = os.path.join(source_folder, patient_id)\n",
    "#         target_path = os.path.join(target_dataset_folder, 'test', fracture_type, patient_id)\n",
    "#         shutil.copytree(source_path, target_path)\n",
    "\n",
    "# print(\"Dataset split completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:\n",
      "aff number: 75 ; nff number: 170\n",
      "train:\n",
      "aff number: 50 ; nff number: 116\n",
      "%: 0.6666666666666666 ; %: 0.6823529411764706\n",
      "test:\n",
      "aff number: 25 ; nff number: 54\n",
      "%: 0.3333333333333333 ; %: 0.3176470588235294\n",
      "Class Weight for AFF (0): 1.6600\n",
      "Class Weight for NFF (1): 0.7155\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_number(path_folder):\n",
    "    # AFF和NFF文件夹路径\n",
    "    aff_folder = os.path.join(path_folder, 'AFF')\n",
    "    nff_folder = os.path.join(path_folder, 'NFF')\n",
    "\n",
    "    # 统计AFF和NFF文件夹中的图片数量\n",
    "    aff_image_count = sum(len(files) for _, _, files in os.walk(aff_folder))\n",
    "    nff_image_count = sum(len(files) for _, _, files in os.walk(nff_folder))\n",
    "    \n",
    "    return aff_image_count, nff_image_count\n",
    "\n",
    "print('total:')\n",
    "path_folder = '/local/data1/honzh073/data/hospital_43'\n",
    "total_aff_num, total_nff_num = count_number(path_folder)\n",
    "print('aff number:', total_aff_num, '; nff number:', total_nff_num)\n",
    "\n",
    "print('train:')\n",
    "path_folder = '/local/data1/honzh073/data/hospital_43_5fold/train'\n",
    "aff_num, nff_num = count_number(path_folder)\n",
    "print('aff number:', aff_num, '; nff number:', nff_num)\n",
    "print('%:', aff_num / total_aff_num , '; %:', nff_num / total_nff_num)\n",
    "\n",
    "print('test:')\n",
    "\n",
    "path_folder = '/local/data1/honzh073/data/hospital_43_5fold/test'\n",
    "aff_num, nff_num = count_number(path_folder)\n",
    "print('aff number:', aff_num, '; nff number:', nff_num)\n",
    "print('%:', aff_num / total_aff_num , '; %:', nff_num / total_nff_num)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "# aff and nff numbers in training dataset\n",
    "train_aff_count = sum(1 for _, label in train_dataset if label == 0)  # 0 AFF\n",
    "train_nff_count = sum(1 for _, label in train_dataset if label == 1)  # 1 NFF\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# Calculate class weights\n",
    "n_aff_train = train_aff_count\n",
    "n_nff_train = train_nff_count\n",
    "\n",
    "class_weight_aff = 1 / (2 * (n_aff_train / (n_aff_train + n_nff_train)))\n",
    "class_weight_nff = 1 / (2 * (n_nff_train / (n_aff_train + n_nff_train)))\n",
    "\n",
    "print(f\"Class Weight for AFF (0): {class_weight_aff:.4f}\")\n",
    "print(f\"Class Weight for NFF (1): {class_weight_nff:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "torch.hub.set_dir('/local/data1/honzh073/download/TORCH_PRETRAINED')\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------\n",
    "# Augmentation\n",
    "# Define data augmentation transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    # transforms.RandomRotation(30),  # Randomly rotate the image up to 30 degrees\n",
    "    # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # Adjust color\n",
    "    # transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Random affine transformation\n",
    "    # transforms.RandomPerspective(),  # Random perspective transformation\n",
    "    # transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Random resized crop\n",
    "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize the image to RGB\n",
    "])\n",
    "\n",
    "# Define the data loaders\n",
    "train_dataset = ImageFolder(root='/local/data1/honzh073/data/hospital_43_5fold/train', transform=transform)\n",
    "test_dataset = ImageFolder(root='/local/data1/honzh073/data/hospital_43_5fold/test', transform=transform)\n",
    "\n",
    "\n",
    "# batch_size = 16\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "    device = torch.device(\"cuda:0\")  # Use GPU 0\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "# ----------------------------------------------------------------------------------------------\n",
    "print('done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/data1/honzh073/anaconda3/envs/myenv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/local/data1/honzh073/anaconda3/envs/myenv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1, Train Loss: 1.1440, Train Acc: 58.39%, Validation Loss: 0.9590, Validation Acc: 52.94%\n",
      "Epoch 2, Train Loss: 0.7752, Train Acc: 64.43%, Validation Loss: 0.6068, Validation Acc: 70.59%\n",
      "Epoch 3, Train Loss: 0.6641, Train Acc: 70.47%, Validation Loss: 0.4458, Validation Acc: 82.35%\n",
      "Epoch 4, Train Loss: 0.5269, Train Acc: 78.52%, Validation Loss: 0.5579, Validation Acc: 70.59%\n",
      "Epoch 5, Train Loss: 0.4550, Train Acc: 82.55%, Validation Loss: 0.6313, Validation Acc: 58.82%\n",
      "Epoch 6, Train Loss: 0.3608, Train Acc: 91.28%, Validation Loss: 0.5130, Validation Acc: 76.47%\n",
      "Epoch 7, Train Loss: 0.3376, Train Acc: 91.95%, Validation Loss: 0.4896, Validation Acc: 88.24%\n",
      "Epoch 8, Train Loss: 0.2888, Train Acc: 91.95%, Validation Loss: 0.3598, Validation Acc: 88.24%\n",
      "Epoch 9, Train Loss: 0.2828, Train Acc: 91.95%, Validation Loss: 0.3525, Validation Acc: 82.35%\n",
      "Epoch 10, Train Loss: 0.1823, Train Acc: 95.30%, Validation Loss: 0.4030, Validation Acc: 82.35%\n",
      "Test Accuracy (Fold 1): 79.75%\n",
      "Fold 2\n",
      "Epoch 1, Train Loss: 0.2524, Train Acc: 90.60%, Validation Loss: 0.0783, Validation Acc: 100.00%\n",
      "Epoch 2, Train Loss: 0.1883, Train Acc: 97.32%, Validation Loss: 0.0854, Validation Acc: 100.00%\n",
      "Epoch 3, Train Loss: 0.1759, Train Acc: 95.97%, Validation Loss: 0.0983, Validation Acc: 100.00%\n",
      "Epoch 4, Train Loss: 0.1468, Train Acc: 95.97%, Validation Loss: 0.0760, Validation Acc: 100.00%\n",
      "Epoch 5, Train Loss: 0.1285, Train Acc: 96.64%, Validation Loss: 0.0424, Validation Acc: 100.00%\n",
      "Epoch 6, Train Loss: 0.1361, Train Acc: 95.97%, Validation Loss: 0.0732, Validation Acc: 100.00%\n",
      "Epoch 7, Train Loss: 0.2056, Train Acc: 93.29%, Validation Loss: 0.0704, Validation Acc: 100.00%\n",
      "Epoch 8, Train Loss: 0.1026, Train Acc: 96.64%, Validation Loss: 0.1187, Validation Acc: 88.24%\n",
      "Epoch 9, Train Loss: 0.1083, Train Acc: 98.66%, Validation Loss: 0.1039, Validation Acc: 94.12%\n",
      "Epoch 10, Train Loss: 0.0598, Train Acc: 97.99%, Validation Loss: 0.1510, Validation Acc: 94.12%\n",
      "Test Accuracy (Fold 2): 82.28%\n",
      "Fold 3\n",
      "Epoch 1, Train Loss: 0.1817, Train Acc: 93.96%, Validation Loss: 0.1084, Validation Acc: 100.00%\n",
      "Epoch 2, Train Loss: 0.1333, Train Acc: 97.32%, Validation Loss: 0.0908, Validation Acc: 100.00%\n",
      "Epoch 3, Train Loss: 0.0677, Train Acc: 97.32%, Validation Loss: 0.0610, Validation Acc: 94.12%\n",
      "Epoch 4, Train Loss: 0.1021, Train Acc: 96.64%, Validation Loss: 0.0482, Validation Acc: 100.00%\n",
      "Epoch 5, Train Loss: 0.0667, Train Acc: 98.66%, Validation Loss: 0.0278, Validation Acc: 100.00%\n",
      "Epoch 6, Train Loss: 0.0407, Train Acc: 98.66%, Validation Loss: 0.0412, Validation Acc: 100.00%\n",
      "Epoch 7, Train Loss: 0.0619, Train Acc: 97.32%, Validation Loss: 0.1694, Validation Acc: 94.12%\n",
      "Epoch 8, Train Loss: 0.0435, Train Acc: 99.33%, Validation Loss: 0.0120, Validation Acc: 100.00%\n",
      "Epoch 9, Train Loss: 0.0159, Train Acc: 100.00%, Validation Loss: 0.0147, Validation Acc: 100.00%\n",
      "Epoch 10, Train Loss: 0.0447, Train Acc: 99.33%, Validation Loss: 0.0060, Validation Acc: 100.00%\n",
      "Test Accuracy (Fold 3): 83.54%\n",
      "Fold 4\n",
      "Epoch 1, Train Loss: 0.0141, Train Acc: 100.00%, Validation Loss: 0.0034, Validation Acc: 100.00%\n",
      "Epoch 2, Train Loss: 0.0224, Train Acc: 98.66%, Validation Loss: 0.0019, Validation Acc: 100.00%\n",
      "Epoch 3, Train Loss: 0.0507, Train Acc: 97.99%, Validation Loss: 0.0279, Validation Acc: 100.00%\n",
      "Epoch 4, Train Loss: 0.0412, Train Acc: 98.66%, Validation Loss: 0.0030, Validation Acc: 100.00%\n",
      "Epoch 5, Train Loss: 0.0722, Train Acc: 97.99%, Validation Loss: 0.0041, Validation Acc: 100.00%\n",
      "Epoch 6, Train Loss: 0.0289, Train Acc: 99.33%, Validation Loss: 0.0104, Validation Acc: 100.00%\n",
      "Epoch 7, Train Loss: 0.0317, Train Acc: 100.00%, Validation Loss: 0.0282, Validation Acc: 100.00%\n",
      "Epoch 8, Train Loss: 0.0527, Train Acc: 98.66%, Validation Loss: 0.0070, Validation Acc: 100.00%\n",
      "Epoch 9, Train Loss: 0.0604, Train Acc: 96.64%, Validation Loss: 0.0053, Validation Acc: 100.00%\n",
      "Epoch 10, Train Loss: 0.1055, Train Acc: 97.32%, Validation Loss: 0.1276, Validation Acc: 100.00%\n",
      "Test Accuracy (Fold 4): 77.22%\n",
      "Fold 5\n",
      "Epoch 1, Train Loss: 0.1119, Train Acc: 95.30%, Validation Loss: 0.0140, Validation Acc: 100.00%\n",
      "Epoch 2, Train Loss: 0.0400, Train Acc: 98.66%, Validation Loss: 0.0100, Validation Acc: 100.00%\n",
      "Epoch 3, Train Loss: 0.0476, Train Acc: 99.33%, Validation Loss: 0.0085, Validation Acc: 100.00%\n",
      "Epoch 4, Train Loss: 0.0231, Train Acc: 100.00%, Validation Loss: 0.0056, Validation Acc: 100.00%\n",
      "Epoch 5, Train Loss: 0.0316, Train Acc: 99.33%, Validation Loss: 0.0073, Validation Acc: 100.00%\n",
      "Epoch 6, Train Loss: 0.0134, Train Acc: 100.00%, Validation Loss: 0.0029, Validation Acc: 100.00%\n",
      "Epoch 7, Train Loss: 0.0524, Train Acc: 98.66%, Validation Loss: 0.0024, Validation Acc: 100.00%\n",
      "Epoch 8, Train Loss: 0.0194, Train Acc: 99.33%, Validation Loss: 0.0040, Validation Acc: 100.00%\n",
      "Epoch 9, Train Loss: 0.0143, Train Acc: 99.33%, Validation Loss: 0.0039, Validation Acc: 100.00%\n",
      "Epoch 10, Train Loss: 0.0197, Train Acc: 99.33%, Validation Loss: 0.0230, Validation Acc: 100.00%\n",
      "Test Accuracy (Fold 5): 79.75%\n",
      "Fold 6\n",
      "Epoch 1, Train Loss: 0.0129, Train Acc: 100.00%, Validation Loss: 0.0060, Validation Acc: 100.00%\n",
      "Epoch 2, Train Loss: 0.0064, Train Acc: 100.00%, Validation Loss: 0.0516, Validation Acc: 100.00%\n",
      "Epoch 3, Train Loss: 0.0176, Train Acc: 99.33%, Validation Loss: 0.0046, Validation Acc: 100.00%\n",
      "Epoch 4, Train Loss: 0.0172, Train Acc: 100.00%, Validation Loss: 0.0118, Validation Acc: 100.00%\n",
      "Epoch 5, Train Loss: 0.0044, Train Acc: 100.00%, Validation Loss: 0.0033, Validation Acc: 100.00%\n",
      "Epoch 6, Train Loss: 0.0152, Train Acc: 100.00%, Validation Loss: 0.0017, Validation Acc: 100.00%\n",
      "Epoch 7, Train Loss: 0.0183, Train Acc: 99.33%, Validation Loss: 0.0010, Validation Acc: 100.00%\n",
      "Epoch 8, Train Loss: 0.0031, Train Acc: 100.00%, Validation Loss: 0.0015, Validation Acc: 100.00%\n",
      "Epoch 9, Train Loss: 0.0059, Train Acc: 100.00%, Validation Loss: 0.0048, Validation Acc: 100.00%\n",
      "Epoch 10, Train Loss: 0.0081, Train Acc: 100.00%, Validation Loss: 0.0017, Validation Acc: 100.00%\n",
      "Test Accuracy (Fold 6): 74.68%\n",
      "Fold 7\n",
      "Epoch 1, Train Loss: 0.0098, Train Acc: 99.33%, Validation Loss: 0.0007, Validation Acc: 100.00%\n",
      "Epoch 2, Train Loss: 0.0013, Train Acc: 100.00%, Validation Loss: 0.0006, Validation Acc: 100.00%\n",
      "Epoch 3, Train Loss: 0.0174, Train Acc: 99.33%, Validation Loss: 0.0004, Validation Acc: 100.00%\n",
      "Epoch 4, Train Loss: 0.0087, Train Acc: 99.33%, Validation Loss: 0.0004, Validation Acc: 100.00%\n",
      "Epoch 5, Train Loss: 0.0051, Train Acc: 100.00%, Validation Loss: 0.0015, Validation Acc: 100.00%\n",
      "Epoch 6, Train Loss: 0.0060, Train Acc: 100.00%, Validation Loss: 0.0005, Validation Acc: 100.00%\n",
      "Epoch 7, Train Loss: 0.0313, Train Acc: 99.33%, Validation Loss: 0.0010, Validation Acc: 100.00%\n",
      "Epoch 8, Train Loss: 0.0051, Train Acc: 100.00%, Validation Loss: 0.0008, Validation Acc: 100.00%\n",
      "Epoch 9, Train Loss: 0.0042, Train Acc: 100.00%, Validation Loss: 0.0013, Validation Acc: 100.00%\n",
      "Epoch 10, Train Loss: 0.0243, Train Acc: 99.33%, Validation Loss: 0.0015, Validation Acc: 100.00%\n",
      "Test Accuracy (Fold 7): 86.08%\n",
      "Fold 8\n",
      "Epoch 1, Train Loss: 0.0201, Train Acc: 99.33%, Validation Loss: 0.0020, Validation Acc: 100.00%\n",
      "Epoch 2, Train Loss: 0.0062, Train Acc: 100.00%, Validation Loss: 0.0353, Validation Acc: 100.00%\n",
      "Epoch 3, Train Loss: 0.0382, Train Acc: 99.33%, Validation Loss: 0.0005, Validation Acc: 100.00%\n",
      "Epoch 4, Train Loss: 0.0024, Train Acc: 100.00%, Validation Loss: 0.0008, Validation Acc: 100.00%\n",
      "Epoch 5, Train Loss: 0.0014, Train Acc: 100.00%, Validation Loss: 0.0020, Validation Acc: 100.00%\n",
      "Epoch 6, Train Loss: 0.0186, Train Acc: 99.33%, Validation Loss: 0.0013, Validation Acc: 100.00%\n",
      "Epoch 7, Train Loss: 0.0220, Train Acc: 99.33%, Validation Loss: 0.0015, Validation Acc: 100.00%\n",
      "Epoch 8, Train Loss: 0.0019, Train Acc: 100.00%, Validation Loss: 0.0006, Validation Acc: 100.00%\n",
      "Epoch 9, Train Loss: 0.0011, Train Acc: 100.00%, Validation Loss: 0.0008, Validation Acc: 100.00%\n",
      "Epoch 10, Train Loss: 0.0034, Train Acc: 100.00%, Validation Loss: 0.0005, Validation Acc: 100.00%\n",
      "Test Accuracy (Fold 8): 81.01%\n",
      "Fold 9\n",
      "Epoch 1, Train Loss: 0.0015, Train Acc: 100.00%, Validation Loss: 0.0004, Validation Acc: 100.00%\n",
      "Epoch 2, Train Loss: 0.0025, Train Acc: 100.00%, Validation Loss: 0.0022, Validation Acc: 100.00%\n",
      "Epoch 3, Train Loss: 0.0012, Train Acc: 100.00%, Validation Loss: 0.0027, Validation Acc: 100.00%\n",
      "Epoch 4, Train Loss: 0.0014, Train Acc: 100.00%, Validation Loss: 0.0006, Validation Acc: 100.00%\n",
      "Epoch 5, Train Loss: 0.0008, Train Acc: 100.00%, Validation Loss: 0.0005, Validation Acc: 100.00%\n",
      "Epoch 6, Train Loss: 0.0054, Train Acc: 100.00%, Validation Loss: 0.0002, Validation Acc: 100.00%\n",
      "Epoch 7, Train Loss: 0.0009, Train Acc: 100.00%, Validation Loss: 0.0002, Validation Acc: 100.00%\n",
      "Epoch 8, Train Loss: 0.0015, Train Acc: 100.00%, Validation Loss: 0.0018, Validation Acc: 100.00%\n",
      "Epoch 9, Train Loss: 0.0009, Train Acc: 100.00%, Validation Loss: 0.0003, Validation Acc: 100.00%\n",
      "Epoch 10, Train Loss: 0.0043, Train Acc: 100.00%, Validation Loss: 0.0011, Validation Acc: 100.00%\n",
      "Test Accuracy (Fold 9): 88.61%\n",
      "Fold 10\n",
      "Epoch 1, Train Loss: 0.0046, Train Acc: 100.00%, Validation Loss: 0.0003, Validation Acc: 100.00%\n",
      "Epoch 2, Train Loss: 0.0005, Train Acc: 100.00%, Validation Loss: 0.0003, Validation Acc: 100.00%\n",
      "Epoch 3, Train Loss: 0.0008, Train Acc: 100.00%, Validation Loss: 0.0007, Validation Acc: 100.00%\n",
      "Epoch 4, Train Loss: 0.0006, Train Acc: 100.00%, Validation Loss: 0.0001, Validation Acc: 100.00%\n",
      "Epoch 5, Train Loss: 0.0014, Train Acc: 100.00%, Validation Loss: 0.0007, Validation Acc: 100.00%\n",
      "Epoch 6, Train Loss: 0.0012, Train Acc: 100.00%, Validation Loss: 0.0003, Validation Acc: 100.00%\n",
      "Epoch 7, Train Loss: 0.0007, Train Acc: 100.00%, Validation Loss: 0.0004, Validation Acc: 100.00%\n",
      "Epoch 8, Train Loss: 0.0006, Train Acc: 100.00%, Validation Loss: 0.0005, Validation Acc: 100.00%\n",
      "Epoch 9, Train Loss: 0.0015, Train Acc: 100.00%, Validation Loss: 0.0004, Validation Acc: 100.00%\n",
      "Epoch 10, Train Loss: 0.0003, Train Acc: 100.00%, Validation Loss: 0.0006, Validation Acc: 100.00%\n",
      "Test Accuracy (Fold 10): 86.08%\n",
      "Average Test Accuracy: 81.90%\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet152, ResNet152_Weights\n",
    "\n",
    "# Define K-fold cross-validator with 5 folds\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Settings\n",
    "lr = 0.0001\n",
    "step_size = 10\n",
    "gamma = 0.1\n",
    "\n",
    "# Class weights\n",
    "class_weights = [class_weight_aff, class_weight_nff]\n",
    "class_weights = torch.Tensor(class_weights).to(device)\n",
    "\n",
    "# ResNet152\n",
    "model = models.resnet152(pretrained=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# Update the output layer\n",
    "model.fc = nn.Linear(2048, 2)  # output layer classes number = dataset classes number\n",
    "model = model.to(device)\n",
    "\n",
    "# Load trained model's weights\n",
    "checkpoint = torch.load('/local/data1/honzh073/saved_model/fracatlas.pth')\n",
    "model.load_state_dict(checkpoint)  # Load the model's state_dict directly\n",
    "\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "\n",
    "# 存储训练和验证的损失和准确度\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "\n",
    "# Loop through the folds\n",
    "for fold, (train_indices, val_indices) in enumerate(kf.split(train_dataset)):\n",
    "    print(f'Fold {fold + 1}')\n",
    "    \n",
    "    # Create data loaders for this fold\n",
    "    train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "    val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, sampler=train_sampler)\n",
    "    val_loader = DataLoader(train_dataset, batch_size=16, sampler=val_sampler)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, pin_memory=True)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(10):  # adjust the number of epochs as needed\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_losses.append(train_loss / total_train)\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_losses.append(val_loss / total_val)\n",
    "        val_accuracy = 100 * correct_val / total_val\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}, Train Loss: {train_loss / total_train:.4f}, Train Acc: {train_accuracy:.2f}%, Validation Loss: {val_loss / total_val:.4f}, Validation Acc: {val_accuracy:.2f}%')\n",
    "\n",
    "    # Evaluate the model on the test set after each fold\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_accuracy = 100 * test_correct / test_total\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "    print(f'Test Accuracy (Fold {fold + 1}): {test_accuracy:.2f}%')\n",
    "\n",
    "# 打印所有折的平均准确度\n",
    "print(f'Average Test Accuracy: {np.mean(test_accuracies):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
