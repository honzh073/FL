{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def split_by_patient(dataset_root, target_root, train_ratio, test_ratio):\n",
    "    if not os.path.exists(target_root):\n",
    "        # create target root if not exit\n",
    "        os.makedirs(target_root)\n",
    "\n",
    "        # AFF and NFF folder\n",
    "        fracture_types = ['0_NFF', '1_AFF' ]\n",
    "        for fracture_type in fracture_types:\n",
    "            source_folder = os.path.join(dataset_root, fracture_type)\n",
    "            patients = os.listdir(source_folder)\n",
    "            \n",
    "            # select random patients\n",
    "            random.seed(123)\n",
    "            random.shuffle(patients)\n",
    "            \n",
    "            # split patients\n",
    "            total_patients = len(patients)\n",
    "            train_end = int(total_patients * train_ratio)\n",
    "            \n",
    "            train_patients = patients[:train_end]\n",
    "            test_patients = patients[train_end:]\n",
    "            \n",
    "            # copy images to folder\n",
    "            for patient_id in train_patients:\n",
    "                source_path = os.path.join(source_folder, patient_id)\n",
    "                target_path = os.path.join(target_root, 'train', fracture_type, patient_id)\n",
    "                shutil.copytree(source_path, target_path)\n",
    "            \n",
    "            for patient_id in test_patients:\n",
    "                source_path = os.path.join(source_folder, patient_id)\n",
    "                target_path = os.path.join(target_root, 'test', fracture_type, patient_id)\n",
    "                shutil.copytree(source_path, target_path)\n",
    "        print(\"Dataset split completed.\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Target dataset folder already exists.\")\n",
    "    \n",
    "    return patients\n",
    "\n",
    "\n",
    "def create_dataset(train_root, test_root):\n",
    "    # Define data augmentation transforms\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=1),\n",
    "        transforms.RandomVerticalFlip(p=1),\n",
    "        transforms.RandomRotation(degrees=(0, 180)),\n",
    "        transforms.ColorJitter(brightness=0.5, contrast=0.1, saturation=0.1, hue=0.3),\n",
    "        # transforms.RandomApply([transforms.RandomAffine(degrees=0, translate=(0.1, 0.1))], p=1),\n",
    "        transforms.GaussianBlur(kernel_size=(5,9), sigma=(0.1, 5)),\n",
    "        transforms.RandomInvert(),\n",
    "        transforms.RandomPosterize(bits=2),\n",
    "        transforms.RandomAdjustSharpness(sharpness_factor=4),\n",
    "        # transforms.RandomAutocontrast(),\n",
    "        transforms.RandomApply([transforms.RandomPerspective(distortion_scale=0.2)], p=1),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # Define the data loaders\n",
    "    train_dataset = ImageFolder(root=train_root, transform=train_transform)\n",
    "    test_dataset = ImageFolder(root=test_root, transform=test_transform)\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def show_image(dataset, num_images=5):\n",
    "    # Get some random indices from the dataset\n",
    "    random_indices = np.random.choice(len(dataset), num_images, replace=False)\n",
    "\n",
    "    # Plot images with truncated names\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, idx in enumerate(random_indices):\n",
    "        image, label = dataset[idx]  # Use the dataset directly\n",
    "        filename = dataset.samples[idx][0]  # Get the filename\n",
    "        truncated_filename = filename.split('/')[-1][:15]  # Extract the last part and truncate to 15 characters\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.title(f\"Label: {label}\")\n",
    "        plt.imshow(image[0], cmap='gray')  # Assuming single-channel (grayscale) image\n",
    "        # plt.imshow(image[0])  # Assuming single-channel (grayscale) image\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def get_classweight(train_dataset):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # aff and nff numbers in training dataset\n",
    "    train_aff_count = sum(1 for _, label in train_dataset if label == 0)  # 0 AFF\n",
    "    train_nff_count = sum(1 for _, label in train_dataset if label == 1)  # 1 NFF\n",
    "\n",
    "    # Calculate class weights\n",
    "    n_aff_train = train_aff_count\n",
    "    n_nff_train = train_nff_count\n",
    "\n",
    "    class_weight_aff = 1 / (2 * (n_aff_train / (n_aff_train + n_nff_train)))\n",
    "    class_weight_nff = 1 / (2 * (n_nff_train / (n_aff_train + n_nff_train)))\n",
    "    \n",
    "    return [class_weight_aff, class_weight_nff]\n",
    "    \n",
    "def train_model(train_loader, validation_loader, classweight, num_epochs, lr, step_size, gamma, model_name, device):\n",
    "\n",
    "    # Load pre-trained model\n",
    "    torch.hub.set_dir('/local/data1/honzh073/download/TORCH_PRETRAINED')\n",
    "    if model_name == 'resnet152':\n",
    "        from torchvision.models import resnet152, ResNet152_Weights\n",
    "        model = models.resnet152(weights=ResNet152_Weights.DEFAULT)\n",
    "        \n",
    "    elif model_name == 'densenet161':\n",
    "        from torchvision.models import densenet161, DenseNet161_Weights\n",
    "        model = models.densenet161(weights=DenseNet161_Weights.DEFAULT)\n",
    "        \n",
    "    elif model_name == 'resnet50':\n",
    "        from torchvision.models import resnet50, ResNet50_Weights\n",
    "        model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "    elif model_name == 'vgg19':\n",
    "        from torchvision.models import vgg19, VGG19_Weights\n",
    "        model = models.vgg19(weights=VGG19_Weights.DEFAULT)\n",
    "\n",
    "    elif model_name == 'resnet101':\n",
    "        from torchvision.models import resnet101, ResNet101_Weights\n",
    "        model = models.resnet101(weights=ResNet101_Weights.DEFAULT)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type. 'vgg19' 'resnet50' 'resnet101' 'resnet152' or 'densenet161'.\")\n",
    "    \n",
    "    # Modify the output layer\n",
    "    num_classes = 2\n",
    "    if model_name == 'densenet161':\n",
    "        in_features = model.classifier.in_features\n",
    "        model.classifier = nn.Sequential(nn.Dropout(0.5),nn.Linear(in_features, num_classes))\n",
    "    else:\n",
    "        in_features = model.fc.in_features\n",
    "        model.fc = nn.Sequential(nn.Dropout(0.5), nn.Linear(in_features, num_classes))\n",
    "    \n",
    "    # Move model to the specified device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.Tensor(classweight).to(device))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    \n",
    "    # Training (loss and accuracy)\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    train_accuracies = []\n",
    "    validation_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "        \n",
    "        model.eval()\n",
    "        correct_validation = 0\n",
    "        total_validation = 0\n",
    "        validation_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in validation_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_validation += labels.size(0)\n",
    "                correct_validation += (predicted == labels).sum().item()\n",
    "                validation_loss += loss.item()\n",
    "        \n",
    "        # validation accuracy and loss\n",
    "        validation_accuracy = 100 * correct_validation / total_validation\n",
    "        validation_loss /= len(validation_loader)\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        validation_losses.append(validation_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        validation_accuracies.append(validation_accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "            f\"Train Loss: {loss.item():.4f}, \"\n",
    "            f\"Validation Loss: {validation_loss:.4f}, \"\n",
    "            f\"Train Acc: {train_accuracy:.2f}%, \"\n",
    "            f\"Validation Acc: {validation_accuracy:.2f}%\")\n",
    "        \n",
    "        scheduler.step()  # Step the learning rate scheduler\n",
    "\n",
    "    # Plot loss and accuracy\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(validation_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(validation_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend() \n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "def test_model(model, test_dataset, batch_size, device):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    model.eval()\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    test_loss = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    test_accuracy = 100 * correct_test / total_test\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.2f}%\")\n",
    "\n",
    "    original_classes = test_loader.dataset.classes\n",
    "\n",
    "    # confusion matrix\n",
    "    with torch.no_grad():\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    auc_score = roc_auc_score(all_labels, all_predictions)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    # Precision、Recall、F1 Score\n",
    "    classification_rep = classification_report(all_labels, all_predictions, target_names=original_classes)\n",
    "    print(\"AUC:\")\n",
    "    print(auc_score)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# create train val and test folder\n",
    "dataset_root = '/local/data1/honzh073/data/hospital_43'\n",
    "target_root = '/local/data1/honzh073/data/hospital_43_data'\n",
    "\n",
    "train_patient_ids = split_by_patient(dataset_root, target_root, train_ratio=0.7, test_ratio=0.3)\n",
    "\n",
    "# create dataset\n",
    "train_root = '/local/data1/honzh073/data/hospital_43_data/train'\n",
    "test_root='/local/data1/honzh073/data/hospital_43_data/test'\n",
    "\n",
    "\n",
    "train_dataset, test_dataset = create_dataset(train_root, test_root)\n",
    "repeated_dataset = torch.utils.data.ConcatDataset([train_dataset] * 10)\n",
    "\n",
    "# show image\n",
    "show_image(train_dataset, 10)\n",
    "\n",
    "# load data\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(repeated_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "# classweight\n",
    "classweight = get_classweight(train_dataset)\n",
    "print(classweight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# 获取所有患者ID\n",
    "train_patient_ids\n",
    "\n",
    "\n",
    "# 定义分层交叉验证\n",
    "def stratified_kfold(dataset, n_splits=5, random_state=42):\n",
    "    X = [i for i in range(len(dataset))]  # 假设每个样本的索引作为X\n",
    "    y = [0 if dataset.samples[i][0].split('/')[-2] == '0_NFF' else 1 for i in range(len(dataset))]  # 0 AFF, 1 NFF\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    return skf.split(X, y)\n",
    "\n",
    "\n",
    "# 进行分层五折交叉验证\n",
    "k = 5\n",
    "for fold, (train_idx, val_idx) in enumerate(stratified_kfold(train_dataset, n_splits=k)):\n",
    "    print(f'Fold {fold + 1}/{k}')\n",
    "\n",
    "    # 构建训练集和验证集的数据加载器\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=torch.utils.data.SubsetRandomSampler(train_idx))\n",
    "    val_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=torch.utils.data.SubsetRandomSampler(val_idx))\n",
    "\n",
    "    # 训练模型\n",
    "    classweight = get_classweight(train_loader.dataset)\n",
    "    model = train_model(train_loader, val_loader, classweight, num_epochs, lr, step_size, gamma, model_name, device)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
