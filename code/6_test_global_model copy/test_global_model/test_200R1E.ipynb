{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All image csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been created and saved to: /local/data1/honzh073/local_repository/FL/code/6_test_global_model/csv_files/all_image.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "def create_csv(image_folder, csv_file_path):\n",
    "    # CSV header\n",
    "    fieldnames = ['HospitalID', 'PatientID', 'ImageID', 'ImagePath', 'Label']\n",
    "    \n",
    "    # open the CSV file in write mode\n",
    "    with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "        # Create a CSV writer object and write the header\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for filename in sorted(os.listdir(image_folder)):\n",
    "            # complete image file path\n",
    "            image_path = os.path.join(image_folder, filename)\n",
    "\n",
    "            # filename to extract HospitalID, PatientID, image number, and image label\n",
    "            parts = filename.split('_')\n",
    "            hospital_id = parts[3]\n",
    "            patient_id = parts[1]\n",
    "            image_number = parts[6]\n",
    "            image_label = parts[4]\n",
    "\n",
    "            # write data into the CSV file\n",
    "            writer.writerow({\n",
    "                'HospitalID': hospital_id,\n",
    "                'PatientID': patient_id,\n",
    "                'ImageID': image_number,\n",
    "                'ImagePath': image_path,\n",
    "                'Label': image_label\n",
    "            })\n",
    "\n",
    "    # created and saved\n",
    "    print(\"CSV file has been created and saved to:\", csv_file_path)\n",
    "\n",
    "\n",
    "image_folder = \"/local/data1/honzh073/data/8bit_down224\"\n",
    "csv_file_path = \"/local/data1/honzh073/local_repository/FL/code/6_test_global_model/csv_files/all_image.csv\"\n",
    "create_csv(image_folder, csv_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset and test 100 times average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "import os\n",
    "import csv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# custom dataset on csv files\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = []\n",
    "        self.labels = []  # Store labels separately\n",
    "        self.patient_ids = []  # Store patient IDs separately\n",
    "        self.transform = transform\n",
    "        \n",
    "        # read csv\n",
    "        with open(csv_file, 'r') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                image_path = row['ImagePath']\n",
    "                label = row['Label']\n",
    "                patient_id = row['PatientID']  # Assuming 'PatientID' is the column name in your CSV file\n",
    "\n",
    "                if label == 'NFF':\n",
    "                    label = 0\n",
    "                elif label == 'AFF':\n",
    "                    label = 1\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid label in CSV file.\")\n",
    "                self.data.append((image_path, label))\n",
    "                self.labels.append(label)\n",
    "                self.patient_ids.append(patient_id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.data[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "def generate_random_pick_dataset(seed, excluded_hospitals, input_csv_path, output_csv_path):\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Read the original CSV file\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    \n",
    "    # Randomly select patients from non-excluded hospitals\n",
    "    selected_data = []\n",
    "    for hospital_id in df['HospitalID'].unique():\n",
    "        if hospital_id not in excluded_hospitals:\n",
    "            patients = df[df['HospitalID'] == hospital_id]['PatientID'].unique()\n",
    "            selected_patients = random.sample(list(patients), min(3, len(patients)))\n",
    "            for patient_id in selected_patients:\n",
    "                patient_data = df[(df['HospitalID'] == hospital_id) & (df['PatientID'] == patient_id)]\n",
    "                selected_data.append(patient_data)\n",
    "    \n",
    "    # Concatenate selected data to create a new DataFrame\n",
    "    selected_df = pd.concat(selected_data)\n",
    "    \n",
    "    # Save the new DataFrame as a CSV file\n",
    "    selected_df.to_csv(output_csv_path, index=False)\n",
    "    \n",
    "    # Return the output file path\n",
    "    return output_csv_path\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "def test_model(model, test_dataset, batch_size):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    model = model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    correct_test_nff = 0\n",
    "    correct_test_aff = 0\n",
    "    total_nff = 0\n",
    "    total_aff = 0\n",
    "    total_test = 0\n",
    "    test_loss = 0\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            total_test += labels.size(0)\n",
    "            total_nff += (labels == 0).sum().item()\n",
    "            total_aff += (labels == 1).sum().item()\n",
    "            correct_test_nff += ((predicted == labels) & (labels == 0)).sum().item()\n",
    "            correct_test_aff += ((predicted == labels) & (labels == 1)).sum().item()\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    test_accuracy_nff = 100 * correct_test_nff / total_nff\n",
    "    test_accuracy_aff = 100 * correct_test_aff / total_aff\n",
    "    test_accuracy_total = 100 * (correct_test_nff + correct_test_aff) / total_test\n",
    "    test_loss /= len(test_loader)\n",
    "    \n",
    "    auc_score = roc_auc_score(all_labels, all_predictions)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "    class_labels = {0: 'NFF', 1: 'AFF'}\n",
    "    classification_rep = classification_report(all_labels, all_predictions, target_names=[class_labels[i] for i in range(len(class_labels))])\n",
    "    \n",
    "    # Store metrics in a dictionary\n",
    "    metrics = {\n",
    "        \"Test Loss\": test_loss,\n",
    "        \"Total Test Accuracy\": test_accuracy_total,\n",
    "        \"Test Accuracy (NFF)\": test_accuracy_nff,\n",
    "        \"Test Accuracy (AFF)\": test_accuracy_aff,\n",
    "        \"AUC\": auc_score,\n",
    "        \"Confusion Matrix\": conf_matrix,\n",
    "        \"Classification Report\": classification_rep\n",
    "    }\n",
    "    return metrics\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# Set random seed for reproducibility\n",
    "def run_100_times(model):\n",
    "    # Define the excluded hospitals and file paths\n",
    "    excluded_hospitals = [18, 43, 55, 100]\n",
    "    input_csv_path = '/local/data1/honzh073/local_repository/FL/code/6_test_global_model/csv_files/all_image.csv'\n",
    "    output_csv_path = '/local/data1/honzh073/local_repository/FL/code/6_test_global_model/csv_files/randompick_patient_data.csv'\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    test_metrics_list = []\n",
    "\n",
    "    # Generate random seeds from 1 to 100\n",
    "    for seed in range(1, 101):\n",
    "        print(f\"Seed: {seed} / 100\", end='\\r')\n",
    "        # Generate random pick dataset\n",
    "        randompick_patient_data_path = generate_random_pick_dataset(seed, excluded_hospitals, input_csv_path, output_csv_path)\n",
    "        \n",
    "        # Load test_dataset using random pick data\n",
    "        test_dataset = CustomDataset(randompick_patient_data_path, transform=test_transform)  # Define your CustomDataset and transform\n",
    "        \n",
    "        # Perform testing and collect metrics\n",
    "        test_metrics = test_model(model, test_dataset, batch_size=8)  # Use your batch size\n",
    "        test_metrics_list.append(test_metrics)\n",
    "\n",
    "    # Calculate mean and standard deviation of metrics\n",
    "    test_loss_values = [metrics['Test Loss'] for metrics in test_metrics_list]\n",
    "    total_accuracy_values = [metrics['Total Test Accuracy'] for metrics in test_metrics_list]\n",
    "    nff_accuracy_values = [metrics['Test Accuracy (NFF)'] for metrics in test_metrics_list]\n",
    "    aff_accuracy_values = [metrics['Test Accuracy (AFF)'] for metrics in test_metrics_list]\n",
    "    auc_values = [metrics['AUC'] for metrics in test_metrics_list]\n",
    "\n",
    "    mean_test_loss = sum(test_loss_values) / len(test_loss_values)\n",
    "    mean_total_accuracy = sum(total_accuracy_values) / len(total_accuracy_values)\n",
    "    mean_nff_accuracy = sum(nff_accuracy_values) / len(nff_accuracy_values)\n",
    "    mean_aff_accuracy = sum(aff_accuracy_values) / len(aff_accuracy_values)\n",
    "    mean_auc = sum(auc_values) / len(auc_values)\n",
    "\n",
    "    std_test_loss = (sum((x - mean_test_loss) ** 2 for x in test_loss_values) / len(test_loss_values)) ** 0.5\n",
    "    std_total_accuracy = (sum((x - mean_total_accuracy) ** 2 for x in total_accuracy_values) / len(total_accuracy_values)) ** 0.5\n",
    "    std_nff_accuracy = (sum((x - mean_nff_accuracy) ** 2 for x in nff_accuracy_values) / len(nff_accuracy_values)) ** 0.5\n",
    "    std_aff_accuracy = (sum((x - mean_aff_accuracy) ** 2 for x in aff_accuracy_values) / len(aff_accuracy_values)) ** 0.5\n",
    "    std_auc = (sum((x - mean_auc) ** 2 for x in auc_values) / len(auc_values)) ** 0.5\n",
    "\n",
    "    # Print mean and standard deviation values\n",
    "    # Print mean and standard deviation values with 4 decimal places\n",
    "    print(\"Mean Test Loss:\", round(mean_test_loss, 4))\n",
    "    print(\"Standard Deviation Test Loss:\", round(std_test_loss, 4))\n",
    "    print(\"Mean Total Accuracy:\", round(mean_total_accuracy, 4))\n",
    "    print(\"Standard Deviation Total Accuracy:\", round(std_total_accuracy, 4))\n",
    "    print(\"Mean NFF Accuracy:\", round(mean_nff_accuracy, 4))\n",
    "    print(\"Standard Deviation NFF Accuracy:\", round(std_nff_accuracy, 4))\n",
    "    print(\"Mean AFF Accuracy:\", round(mean_aff_accuracy, 4))\n",
    "    print(\"Standard Deviation AFF Accuracy:\", round(std_aff_accuracy, 4))\n",
    "    print(\"Mean AUC:\", round(mean_auc, 4))\n",
    "    print(\"Standard Deviation AUC:\", round(std_auc, 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test Loss: 0.5736\n",
      "Standard Deviation Test Loss: 0.0977\n",
      "Mean Total Accuracy: 88.1745\n",
      "Standard Deviation Total Accuracy: 1.4956\n",
      "Mean NFF Accuracy: 91.6996\n",
      "Standard Deviation NFF Accuracy: 1.3189\n",
      "Mean AFF Accuracy: 70.8532\n",
      "Standard Deviation AFF Accuracy: 4.7077\n",
      "Mean AUC: 0.8128\n",
      "Standard Deviation AUC: 0.0245\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision.models import vgg19, VGG19_Weights\n",
    "\n",
    "torch.hub.set_dir('/local/data1/honzh073/download/TORCH_PRETRAINED')\n",
    "\n",
    "# models from job, 50% freezed\n",
    "class CustomNet(nn.Module):\n",
    "    def __init__(self, num_classes=2, freeze_percentage=0.5):\n",
    "        super(CustomNet, self).__init__()\n",
    "\n",
    "        # Load the pre-trained VGG19 model\n",
    "        vgg19_model = models.vgg19(weights=VGG19_Weights.DEFAULT)\n",
    "\n",
    "        # Extract features (all layers except the classifier)\n",
    "        self.features = nn.Sequential(*list(vgg19_model.features.children()))\n",
    "\n",
    "        # Calculate the index where to split the layers\n",
    "        total_layers = len(list(self.features.children()))\n",
    "        split_idx = int(total_layers * freeze_percentage)\n",
    "\n",
    "        # Split the layers into groups for freezing and non-freezing\n",
    "        children = list(self.features.children())\n",
    "        self.frozen_features = nn.Sequential(*children[:split_idx])\n",
    "        self.unfrozen_features = nn.Sequential(*children[split_idx:])\n",
    "\n",
    "        # Freeze layers in self.frozen_features\n",
    "        for param in self.frozen_features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Define the final fully connected layer\n",
    "        in_features = vgg19_model.classifier[0].in_features\n",
    "        self.classifier = nn.Linear(in_features, num_classes)  # VGG19's last feature map has 512 channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.frozen_features(x)\n",
    "        x = self.unfrozen_features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# create net\n",
    "model = CustomNet(num_classes=2)  # num_classes 2\n",
    "\n",
    "# checkpoint\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# vgg19_freeze_Adam_200R_1E\n",
    "global_path = '/local/data1/honzh073/data/suzuki.ad.liu.se/transfer/vgg19_freeze_Adam_200R_1E/workspace/app_server/FL_global_model.pt'\n",
    "\n",
    "checkpoint = torch.load(global_path, map_location=device)\n",
    "\n",
    "# print(checkpoint.keys())\n",
    "\n",
    "# modify checkpoint keys，match model\n",
    "# load model weights\n",
    "state_dict = checkpoint['model']\n",
    "modified_state_dict = {}\n",
    "for key, value in state_dict.items():\n",
    "    # modify key names，match model layers\n",
    "    new_key = key.replace(\"model.\", \"\")\n",
    "    modified_state_dict[new_key] = value\n",
    "\n",
    "# load weights to model\n",
    "model.load_state_dict(modified_state_dict)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "run_100_times(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test Loss: 0.3845\n",
      "Standard Deviation Test Loss: 0.0416\n",
      "Mean Total Accuracy: 87.4984\n",
      "Standard Deviation Total Accuracy: 1.4084\n",
      "Mean NFF Accuracy: 91.8517\n",
      "Standard Deviation NFF Accuracy: 1.0737\n",
      "Mean AFF Accuracy: 66.1412\n",
      "Standard Deviation AFF Accuracy: 5.0764\n",
      "Mean AUC: 0.79\n",
      "Standard Deviation AUC: 0.0263\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision.models import resnet101, ResNet101_Weights\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "torch.hub.set_dir('/local/data1/honzh073/download/TORCH_PRETRAINED')\n",
    "\n",
    "class CustomNet(nn.Module):\n",
    "    def __init__(self, num_classes=2, freeze_percentage=0.5):\n",
    "        super(CustomNet, self).__init__()\n",
    "\n",
    "        self.features = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "        # Calculate the index where to split the layerszh\n",
    "        total_layers = len(list(self.features.children()))\n",
    "        split_idx = int(total_layers * freeze_percentage)\n",
    "\n",
    "        # Split the layers into groups for freezing and non-freezing\n",
    "        children = list(self.features.children())\n",
    "        self.frozen_features = nn.Sequential(*children[:split_idx])\n",
    "        self.unfrozen_features = nn.Sequential(*children[split_idx:-1])  # Exclude the last layer\n",
    "\n",
    "        # Freeze layers in self.frozen_features\n",
    "        for param in self.frozen_features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Define the final fully connected layer\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.frozen_features(x)\n",
    "        x = self.unfrozen_features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "# create net\n",
    "model = CustomNet(num_classes=2)  # num_classes 2\n",
    "\n",
    "# checkpoint\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# resnet50_freeze_Adam_200R_1E\n",
    "global_path = '/local/data1/honzh073/data/suzuki.ad.liu.se/transfer/resnet50_freeze_Adam_200R_1E/workspace/app_server/FL_global_model.pt'\n",
    "\n",
    "checkpoint = torch.load(global_path, map_location=device)\n",
    "\n",
    "# print(checkpoint.keys())\n",
    "\n",
    "# modify checkpoint keys，match model\n",
    "# load model weights\n",
    "state_dict = checkpoint['model']\n",
    "modified_state_dict = {}\n",
    "for key, value in state_dict.items():\n",
    "    # modify key names，match model layers\n",
    "    new_key = key.replace(\"model.\", \"\")\n",
    "    modified_state_dict[new_key] = value\n",
    "\n",
    "# load weights to model\n",
    "model.load_state_dict(modified_state_dict)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "run_100_times(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test Loss: 0.5193\n",
      "Standard Deviation Test Loss: 0.0929\n",
      "Mean Total Accuracy: 89.291\n",
      "Standard Deviation Total Accuracy: 1.3926\n",
      "Mean NFF Accuracy: 94.1199\n",
      "Standard Deviation NFF Accuracy: 1.0247\n",
      "Mean AFF Accuracy: 65.5622\n",
      "Standard Deviation AFF Accuracy: 4.7336\n",
      "Mean AUC: 0.7984\n",
      "Standard Deviation AUC: 0.0239\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision.models import resnet101, ResNet101_Weights\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "torch.hub.set_dir('/local/data1/honzh073/download/TORCH_PRETRAINED')\n",
    "\n",
    "class CustomNet(nn.Module):\n",
    "    def __init__(self, num_classes=2, freeze_percentage=0.5):\n",
    "        super(CustomNet, self).__init__()\n",
    "\n",
    "        self.features = models.resnet101(weights=ResNet101_Weights.DEFAULT)\n",
    "\n",
    "        # Calculate the index where to split the layers\n",
    "        total_layers = len(list(self.features.children()))\n",
    "        split_idx = int(total_layers * freeze_percentage)\n",
    "\n",
    "        # Split the layers into groups for freezing and non-freezing\n",
    "        children = list(self.features.children())\n",
    "        self.frozen_features = nn.Sequential(*children[:split_idx])\n",
    "        self.unfrozen_features = nn.Sequential(*children[split_idx:-1])  # Exclude the last layer\n",
    "\n",
    "        # Freeze layers in self.frozen_features\n",
    "        for param in self.frozen_features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Define the final fully connected layer\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.frozen_features(x)\n",
    "        x = self.unfrozen_features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "# create net\n",
    "model = CustomNet(num_classes=2)  # num_classes 2\n",
    "\n",
    "# checkpoint\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# resnet101_freeze_Adam_200R_1E\n",
    "global_path = '/local/data1/honzh073/data/suzuki.ad.liu.se/transfer/resnet101_freeze_Adam_200R_1E/workspace/app_server/FL_global_model.pt'\n",
    "\n",
    "checkpoint = torch.load(global_path, map_location=device)\n",
    "\n",
    "# print(checkpoint.keys())\n",
    "\n",
    "# modify checkpoint keys，match model\n",
    "# load model weights\n",
    "state_dict = checkpoint['model']\n",
    "modified_state_dict = {}\n",
    "for key, value in state_dict.items():\n",
    "    # modify key names，match model layers\n",
    "    new_key = key.replace(\"model.\", \"\")\n",
    "    modified_state_dict[new_key] = value\n",
    "\n",
    "# load weights to model\n",
    "model.load_state_dict(modified_state_dict)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "run_100_times(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test Loss: 0.3875\n",
      "Standard Deviation Test Loss: 0.0741\n",
      "Mean Total Accuracy: 91.1242\n",
      "Standard Deviation Total Accuracy: 1.3936\n",
      "Mean NFF Accuracy: 95.3207\n",
      "Standard Deviation NFF Accuracy: 0.8778\n",
      "Mean AFF Accuracy: 70.5477\n",
      "Standard Deviation AFF Accuracy: 5.448\n",
      "Mean AUC: 0.8293\n",
      "Standard Deviation AUC: 0.0281\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision.models import resnet152, ResNet152_Weights\n",
    "\n",
    "torch.hub.set_dir('/local/data1/honzh073/download/TORCH_PRETRAINED')\n",
    "\n",
    "class CustomNet(nn.Module):\n",
    "    def __init__(self, num_classes=2, freeze_percentage=0.5):\n",
    "        super(CustomNet, self).__init__()\n",
    "\n",
    "        self.features = models.resnet152(weights=ResNet152_Weights.DEFAULT)\n",
    "\n",
    "        # Calculate the index where to split the layers\n",
    "        total_layers = len(list(self.features.children()))\n",
    "        split_idx = int(total_layers * freeze_percentage)\n",
    "\n",
    "        # Split the layers into groups for freezing and non-freezing\n",
    "        children = list(self.features.children())\n",
    "        self.frozen_features = nn.Sequential(*children[:split_idx])\n",
    "        self.unfrozen_features = nn.Sequential(*children[split_idx:-1])  # Exclude the last layer\n",
    "\n",
    "        # Freeze layers in self.frozen_features\n",
    "        for param in self.frozen_features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Define the final fully connected layer\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.frozen_features(x)\n",
    "        x = self.unfrozen_features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "# create net\n",
    "model = CustomNet(num_classes=2)  # num_classes 2\n",
    "\n",
    "# checkpoint\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# resnet152_freeze_Adam_200R_1E\n",
    "global_path = '/local/data1/honzh073/data/suzuki.ad.liu.se/34e00b5d-518e-42d5-8fe3-0f0ee5b44959/app_server/FL_global_model.pt'\n",
    "\n",
    "checkpoint = torch.load(global_path, map_location=device)\n",
    "\n",
    "# print(checkpoint.keys())\n",
    "\n",
    "# modify checkpoint keys，match model\n",
    "# load model weights\n",
    "state_dict = checkpoint['model']\n",
    "modified_state_dict = {}\n",
    "for key, value in state_dict.items():\n",
    "    # modify key names，match model layers\n",
    "    new_key = key.replace(\"model.\", \"\")\n",
    "    modified_state_dict[new_key] = value\n",
    "\n",
    "# load weights to model\n",
    "model.load_state_dict(modified_state_dict)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "run_100_times(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet 161\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test Loss: 0.4327\n",
      "Standard Deviation Test Loss: 0.0272\n",
      "Mean Total Accuracy: 83.8643\n",
      "Standard Deviation Total Accuracy: 2.198\n",
      "Mean NFF Accuracy: 98.1343\n",
      "Standard Deviation NFF Accuracy: 0.415\n",
      "Mean AFF Accuracy: 13.9229\n",
      "Standard Deviation AFF Accuracy: 2.6984\n",
      "Mean AUC: 0.5603\n",
      "Standard Deviation AUC: 0.0139\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision.models import densenet161, DenseNet161_Weights\n",
    "\n",
    "torch.hub.set_dir('/local/data1/honzh073/download/TORCH_PRETRAINED')\n",
    "\n",
    "class CustomNet(nn.Module):\n",
    "    def __init__(self, num_classes=2, freeze_percentage=0.5):\n",
    "        super(CustomNet, self).__init__()\n",
    "\n",
    "        self.features = models.densenet161(weights=DenseNet161_Weights.DEFAULT)\n",
    "\n",
    "        # Calculate the index where to split the layers\n",
    "        total_layers = len(list(self.features.children()))\n",
    "        split_idx = int(total_layers * freeze_percentage)\n",
    "\n",
    "        # Split the layers into groups for freezing and non-freezing\n",
    "        children = list(self.features.children())\n",
    "        self.frozen_features = nn.Sequential(*children[:split_idx])\n",
    "        self.unfrozen_features = nn.Sequential(*children[split_idx:-1])  # Exclude the last layer\n",
    "\n",
    "        # Freeze layers in self.frozen_features\n",
    "        for param in self.frozen_features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Define the final fully connected layer\n",
    "        self.fc = nn.Linear(2208, num_classes)  # DenseNet-161 has 2208 output features\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.frozen_features(x)\n",
    "        x = self.unfrozen_features(x)\n",
    "        x = nn.functional.adaptive_avg_pool2d(x, (1, 1))  # Global average pooling\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# create net\n",
    "model = CustomNet(num_classes=2)  # num_classes 2\n",
    "\n",
    "# checkpoint\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# resnet101_freeze_Adam_200R_1E\n",
    "global_path = '/local/data1/honzh073/data/suzuki.ad.liu.se/transfer/densenet161_freeze_Adam_200R1E/workspace/app_server/FL_global_model.pt'\n",
    "\n",
    "checkpoint = torch.load(global_path, map_location=device)\n",
    "\n",
    "# print(checkpoint.keys())\n",
    "\n",
    "# modify checkpoint keys，match model\n",
    "# load model weights\n",
    "state_dict = checkpoint['model']\n",
    "modified_state_dict = {}\n",
    "for key, value in state_dict.items():\n",
    "    # modify key names，match model layers\n",
    "    new_key = key.replace(\"model.\", \"\")\n",
    "    modified_state_dict[new_key] = value\n",
    "\n",
    "# load weights to model\n",
    "model.load_state_dict(modified_state_dict)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "run_100_times(model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
