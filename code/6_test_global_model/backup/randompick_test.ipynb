{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All image csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been created and saved to: /local/data1/honzh073/local_repository/FL/code/6_test_global_model/csv_files/all_image.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "def create_csv(image_folder, csv_file_path):\n",
    "    # CSV header\n",
    "    fieldnames = ['HospitalID', 'PatientID', 'ImageID', 'ImagePath', 'Label']\n",
    "    \n",
    "    # open the CSV file in write mode\n",
    "    with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "        # Create a CSV writer object and write the header\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for filename in sorted(os.listdir(image_folder)):\n",
    "            # complete image file path\n",
    "            image_path = os.path.join(image_folder, filename)\n",
    "\n",
    "            # filename to extract HospitalID, PatientID, image number, and image label\n",
    "            parts = filename.split('_')\n",
    "            hospital_id = parts[3]\n",
    "            patient_id = parts[1]\n",
    "            image_number = parts[6]\n",
    "            image_label = parts[4]\n",
    "\n",
    "            # write data into the CSV file\n",
    "            writer.writerow({\n",
    "                'HospitalID': hospital_id,\n",
    "                'PatientID': patient_id,\n",
    "                'ImageID': image_number,\n",
    "                'ImagePath': image_path,\n",
    "                'Label': image_label\n",
    "            })\n",
    "\n",
    "    # created and saved\n",
    "    print(\"CSV file has been created and saved to:\", csv_file_path)\n",
    "\n",
    "\n",
    "image_folder = \"/local/data1/honzh073/data/8bit_down224\"\n",
    "csv_file_path = \"/local/data1/honzh073/local_repository/FL/code/6_test_global_model/csv_files/all_image.csv\"\n",
    "create_csv(image_folder, csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def generate_random_pick_dataset(seed, excluded_hospitals, input_csv_path, output_csv_path, num_aff=500, num_nff=500):\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Read the input CSV file\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "\n",
    "    # Filter out excluded hospitals\n",
    "    df = df[~df['HospitalID'].isin(excluded_hospitals)]\n",
    "\n",
    "    # Separate AFF and NFF images\n",
    "    aff_images = df[df['Label'] == 'AFF']\n",
    "    nff_images = df[df['Label'] == 'NFF']\n",
    "\n",
    "    # Randomly pick images from each category\n",
    "    random_aff_images = aff_images.sample(n=num_aff, random_state=seed)\n",
    "    random_nff_images = nff_images.sample(n=num_nff, random_state=seed)\n",
    "\n",
    "    # Concatenate the randomly picked images\n",
    "    random_pick_dataset = pd.concat([random_aff_images, random_nff_images])\n",
    "\n",
    "    # Save the result to a new CSV file\n",
    "    random_pick_dataset.to_csv(output_csv_path, index=False)\n",
    "\n",
    "    return output_csv_path\n",
    "\n",
    "# 使用 generate_random_pick_dataset 函数\n",
    "seed = 1  # 选择一个随机种子\n",
    "input_csv_path = '/local/data1/honzh073/local_repository/FL/code/6_test_global_model/csv_files/all_image.csv'\n",
    "\n",
    "output_csv_path = '/local/data1/honzh073/local_repository/FL/code/6_test_global_model/csv_files/randompick_patient_data.csv'\n",
    "excluded_hospitals = [18, 43, 55, 100]\n",
    "\n",
    "randompick_patient_data_path = generate_random_pick_dataset(seed, excluded_hospitals, input_csv_path, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset and test 100 times average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "import os\n",
    "import csv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# custom dataset on csv files\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = []\n",
    "        self.labels = []  # Store labels separately\n",
    "        self.patient_ids = []  # Store patient IDs separately\n",
    "        self.transform = transform\n",
    "        \n",
    "        # read csv\n",
    "        with open(csv_file, 'r') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                image_path = row['ImagePath']\n",
    "                label = row['Label']\n",
    "                patient_id = row['PatientID']  # Assuming 'PatientID' is the column name in your CSV file\n",
    "\n",
    "                if label == 'NFF':\n",
    "                    label = 0\n",
    "                elif label == 'AFF':\n",
    "                    label = 1\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid label in CSV file.\")\n",
    "                self.data.append((image_path, label))\n",
    "                self.labels.append(label)\n",
    "                self.patient_ids.append(patient_id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.data[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "def generate_random_pick_dataset(seed, excluded_hospitals, input_csv_path, output_csv_path):\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Read the original CSV file\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    \n",
    "    # Randomly select patients from non-excluded hospitals\n",
    "    selected_data = []\n",
    "    for hospital_id in df['HospitalID'].unique():\n",
    "        if hospital_id not in excluded_hospitals:\n",
    "            patients = df[df['HospitalID'] == hospital_id]['PatientID'].unique()\n",
    "            selected_patients = random.sample(list(patients), min(3, len(patients)))\n",
    "            for patient_id in selected_patients:\n",
    "                patient_data = df[(df['HospitalID'] == hospital_id) & (df['PatientID'] == patient_id)]\n",
    "                selected_data.append(patient_data)\n",
    "    \n",
    "    # Concatenate selected data to create a new DataFrame\n",
    "    selected_df = pd.concat(selected_data)\n",
    "    \n",
    "    # Save the new DataFrame as a CSV file\n",
    "    selected_df.to_csv(output_csv_path, index=False)\n",
    "    \n",
    "    # Return the output file path\n",
    "    return output_csv_path\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "def test_model(model, test_dataset, batch_size):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    model = model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    correct_test_nff = 0\n",
    "    correct_test_aff = 0\n",
    "    total_nff = 0\n",
    "    total_aff = 0\n",
    "    total_test = 0\n",
    "    test_loss = 0\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            total_test += labels.size(0)\n",
    "            total_nff += (labels == 0).sum().item()\n",
    "            total_aff += (labels == 1).sum().item()\n",
    "            correct_test_nff += ((predicted == labels) & (labels == 0)).sum().item()\n",
    "            correct_test_aff += ((predicted == labels) & (labels == 1)).sum().item()\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    test_accuracy_nff = 100 * correct_test_nff / total_nff\n",
    "    test_accuracy_aff = 100 * correct_test_aff / total_aff\n",
    "    test_accuracy_total = 100 * (correct_test_nff + correct_test_aff) / total_test\n",
    "    test_loss /= len(test_loader)\n",
    "    \n",
    "    auc_score = roc_auc_score(all_labels, all_predictions)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "    class_labels = {0: 'NFF', 1: 'AFF'}\n",
    "    classification_rep = classification_report(all_labels, all_predictions, target_names=[class_labels[i] for i in range(len(class_labels))])\n",
    "    \n",
    "    # Store metrics in a dictionary\n",
    "    metrics = {\n",
    "        \"Test Loss\": test_loss,\n",
    "        \"Total Test Accuracy\": test_accuracy_total,\n",
    "        \"Test Accuracy (NFF)\": test_accuracy_nff,\n",
    "        \"Test Accuracy (AFF)\": test_accuracy_aff,\n",
    "        \"AUC\": auc_score,\n",
    "        \"Confusion Matrix\": conf_matrix,\n",
    "        \"Classification Report\": classification_rep\n",
    "    }\n",
    "    return metrics\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# Set random seed for reproducibility\n",
    "def run_10_times(model):\n",
    "    # Define the excluded hospitals and file paths\n",
    "    excluded_hospitals = [18, 43, 55, 100]\n",
    "    input_csv_path = '/local/data1/honzh073/local_repository/FL/code/6_test_global_model/csv_files/all_image.csv'\n",
    "    output_csv_path = '/local/data1/honzh073/local_repository/FL/code/6_test_global_model/csv_files/randompick_patient_data.csv'\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    test_metrics_list = []\n",
    "\n",
    "    # Generate random seeds from 1 to 100\n",
    "    for seed in range(10):\n",
    "        print(f\"Seed: {seed} / 10\", end='\\r')\n",
    "        # Generate random pick dataset\n",
    "        randompick_patient_data_path = generate_random_pick_dataset(seed, excluded_hospitals, input_csv_path, output_csv_path)\n",
    "        \n",
    "        # Load test_dataset using random pick data\n",
    "        test_dataset = CustomDataset(randompick_patient_data_path, transform=test_transform)  # Define your CustomDataset and transform\n",
    "        \n",
    "        # Perform testing and collect metrics\n",
    "        test_metrics = test_model(model, test_dataset, batch_size=8)  # Use your batch size\n",
    "        test_metrics_list.append(test_metrics)\n",
    "\n",
    "    # Calculate mean and standard deviation of metrics\n",
    "    test_loss_values = [metrics['Test Loss'] for metrics in test_metrics_list]\n",
    "    total_accuracy_values = [metrics['Total Test Accuracy'] for metrics in test_metrics_list]\n",
    "    nff_accuracy_values = [metrics['Test Accuracy (NFF)'] for metrics in test_metrics_list]\n",
    "    aff_accuracy_values = [metrics['Test Accuracy (AFF)'] for metrics in test_metrics_list]\n",
    "    auc_values = [metrics['AUC'] for metrics in test_metrics_list]\n",
    "\n",
    "    mean_test_loss = sum(test_loss_values) / len(test_loss_values)\n",
    "    mean_total_accuracy = sum(total_accuracy_values) / len(total_accuracy_values)\n",
    "    mean_nff_accuracy = sum(nff_accuracy_values) / len(nff_accuracy_values)\n",
    "    mean_aff_accuracy = sum(aff_accuracy_values) / len(aff_accuracy_values)\n",
    "    mean_auc = sum(auc_values) / len(auc_values)\n",
    "\n",
    "    std_test_loss = (sum((x - mean_test_loss) ** 2 for x in test_loss_values) / len(test_loss_values)) ** 0.5\n",
    "    std_total_accuracy = (sum((x - mean_total_accuracy) ** 2 for x in total_accuracy_values) / len(total_accuracy_values)) ** 0.5\n",
    "    std_nff_accuracy = (sum((x - mean_nff_accuracy) ** 2 for x in nff_accuracy_values) / len(nff_accuracy_values)) ** 0.5\n",
    "    std_aff_accuracy = (sum((x - mean_aff_accuracy) ** 2 for x in aff_accuracy_values) / len(aff_accuracy_values)) ** 0.5\n",
    "    std_auc = (sum((x - mean_auc) ** 2 for x in auc_values) / len(auc_values)) ** 0.5\n",
    "\n",
    "    # Print mean and standard deviation values\n",
    "    # Print mean and standard deviation values with 4 decimal places\n",
    "    print(\"Mean Test Loss:\", round(mean_test_loss, 4))\n",
    "    print(\"Standard Deviation Test Loss:\", round(std_test_loss, 4))\n",
    "    print(\"Mean Total Accuracy:\", round(mean_total_accuracy, 4))\n",
    "    print(\"Standard Deviation Total Accuracy:\", round(std_total_accuracy, 4))\n",
    "    print(\"Mean NFF Accuracy:\", round(mean_nff_accuracy, 4))\n",
    "    print(\"Standard Deviation NFF Accuracy:\", round(std_nff_accuracy, 4))\n",
    "    print(\"Mean AFF Accuracy:\", round(mean_aff_accuracy, 4))\n",
    "    print(\"Standard Deviation AFF Accuracy:\", round(std_aff_accuracy, 4))\n",
    "    print(\"Mean AUC:\", round(mean_auc, 4))\n",
    "    print(\"Standard Deviation AUC:\", round(std_auc, 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test Loss: 0.5775\n",
      "Standard Deviation Test Loss: 0.0704\n",
      "Mean Total Accuracy: 87.5233\n",
      "Standard Deviation Total Accuracy: 1.5103\n",
      "Mean NFF Accuracy: 89.5664\n",
      "Standard Deviation NFF Accuracy: 1.0649\n",
      "Mean AFF Accuracy: 77.1317\n",
      "Standard Deviation AFF Accuracy: 6.1774\n",
      "Mean AUC: 0.8335\n",
      "Standard Deviation AUC: 0.0324\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision.models import vgg19, VGG19_Weights\n",
    "\n",
    "torch.hub.set_dir('/local/data1/honzh073/download/TORCH_PRETRAINED')\n",
    "\n",
    "# models from job, 50% freezed\n",
    "class CustomNet(nn.Module):\n",
    "    def __init__(self, num_classes=2, freeze_percentage=0.5):\n",
    "        super(CustomNet, self).__init__()\n",
    "\n",
    "        # Load the pre-trained VGG19 model\n",
    "        vgg19_model = models.vgg19(weights=VGG19_Weights.DEFAULT)\n",
    "\n",
    "        # Extract features (all layers except the classifier)\n",
    "        self.features = nn.Sequential(*list(vgg19_model.features.children()))\n",
    "\n",
    "        # Calculate the index where to split the layers\n",
    "        total_layers = len(list(self.features.children()))\n",
    "        split_idx = int(total_layers * freeze_percentage)\n",
    "\n",
    "        # Split the layers into groups for freezing and non-freezing\n",
    "        children = list(self.features.children())\n",
    "        self.frozen_features = nn.Sequential(*children[:split_idx])\n",
    "        self.unfrozen_features = nn.Sequential(*children[split_idx:])\n",
    "\n",
    "        # Freeze layers in self.frozen_features\n",
    "        for param in self.frozen_features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Define the final fully connected layer\n",
    "        in_features = vgg19_model.classifier[0].in_features\n",
    "        self.classifier = nn.Linear(in_features, num_classes)  # VGG19's last feature map has 512 channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.frozen_features(x)\n",
    "        x = self.unfrozen_features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# create net\n",
    "model = CustomNet(num_classes=2)  # num_classes 2\n",
    "\n",
    "# checkpoint\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# vgg19_freeze_Adam_100R_2E\n",
    "global_path = '/local/data1/honzh073/data/suzuki.ad.liu.se/8851362f-e38d-4f9b-91b6-2cb42da00d67/app_server/FL_global_model.pt'\n",
    "checkpoint = torch.load(global_path, map_location=device)\n",
    "\n",
    "# print(checkpoint.keys())\n",
    "\n",
    "# modify checkpoint keys，match model\n",
    "# load model weights\n",
    "state_dict = checkpoint['model']\n",
    "modified_state_dict = {}\n",
    "for key, value in state_dict.items():\n",
    "    # modify key names，match model layers\n",
    "    new_key = key.replace(\"model.\", \"\")\n",
    "    modified_state_dict[new_key] = value\n",
    "\n",
    "# load weights to model\n",
    "model.load_state_dict(modified_state_dict)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFF: 500, ratio: 0.50\n",
      "NFF: 500, ratio: 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/data1/honzh073/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/local/data1/honzh073/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/local/data1/honzh073/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Test Loss': nan,\n",
       " 'Total Test Accuracy': 50.0,\n",
       " 'Test Accuracy (NFF)': 100.0,\n",
       " 'Test Accuracy (AFF)': 0.0,\n",
       " 'AUC': 0.5,\n",
       " 'Confusion Matrix': array([[500,   0],\n",
       "        [500,   0]]),\n",
       " 'Classification Report': '              precision    recall  f1-score   support\\n\\n         NFF       0.50      1.00      0.67       500\\n         AFF       0.00      0.00      0.00       500\\n\\n    accuracy                           0.50      1000\\n   macro avg       0.25      0.50      0.33      1000\\nweighted avg       0.25      0.50      0.33      1000\\n'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision.models import resnet101, ResNet101_Weights\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "torch.hub.set_dir('/local/data1/honzh073/download/TORCH_PRETRAINED')\n",
    "\n",
    "class CustomNet(nn.Module):\n",
    "    def __init__(self, num_classes=2, freeze_percentage=0.5):\n",
    "        super(CustomNet, self).__init__()\n",
    "\n",
    "        self.features = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "        # Calculate the index where to split the layerszh\n",
    "        total_layers = len(list(self.features.children()))\n",
    "        split_idx = int(total_layers * freeze_percentage)\n",
    "\n",
    "        # Split the layers into groups for freezing and non-freezing\n",
    "        children = list(self.features.children())\n",
    "        self.frozen_features = nn.Sequential(*children[:split_idx])\n",
    "        self.unfrozen_features = nn.Sequential(*children[split_idx:-1])  # Exclude the last layer\n",
    "\n",
    "        # Freeze layers in self.frozen_features\n",
    "        for param in self.frozen_features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Define the final fully connected layer\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.frozen_features(x)\n",
    "        x = self.unfrozen_features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "# create net\n",
    "model = CustomNet(num_classes=2)  # num_classes 2\n",
    "\n",
    "# checkpoint\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "global_path = '/local/data1/honzh073/data/suzuki.ad.liu.se/c8754d06-15de-40e6-981a-bb9709ad25a6/app_server/FL_global_model.pt'\n",
    "\n",
    "checkpoint = torch.load(global_path, map_location=device)\n",
    "\n",
    "# print(checkpoint.keys())\n",
    "\n",
    "# modify checkpoint keys，match model\n",
    "# load model weights\n",
    "state_dict = checkpoint['model']\n",
    "modified_state_dict = {}\n",
    "for key, value in state_dict.items():\n",
    "    # modify key names，match model layers\n",
    "    new_key = key.replace(\"model.\", \"\")\n",
    "    modified_state_dict[new_key] = value\n",
    "\n",
    "# load weights to model\n",
    "model.load_state_dict(modified_state_dict)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Create an instance of CustomDataset for testing\n",
    "test_dataset = CustomDataset('/local/data1/honzh073/local_repository/FL/code/6_test_global_model/csv_files/randompick_patient_data.csv', transform=test_transform)\n",
    "\n",
    "\n",
    "# For test dataset\n",
    "test_NFF_count = sum(1 for _, label in test_dataset if label == 0)  # 0 NFF\n",
    "test_AFF_count = sum(1 for _, label in test_dataset if label == 1)  # 1 AFF\n",
    "print(f\"AFF: {test_AFF_count}, ratio: {test_AFF_count / (test_AFF_count + test_NFF_count):.2f}\")\n",
    "print(f\"NFF: {test_NFF_count}, ratio: {test_NFF_count / (test_AFF_count + test_NFF_count):.2f}\")\n",
    "\n",
    "test_model(model, test_dataset=test_dataset, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test Loss: 0.4462\n",
      "Standard Deviation Test Loss: 0.1381\n",
      "Mean Total Accuracy: 91.2144\n",
      "Standard Deviation Total Accuracy: 1.7327\n",
      "Mean NFF Accuracy: 96.4897\n",
      "Standard Deviation NFF Accuracy: 0.9843\n",
      "Mean AFF Accuracy: 64.274\n",
      "Standard Deviation AFF Accuracy: 6.9001\n",
      "Mean AUC: 0.8038\n",
      "Standard Deviation AUC: 0.0364\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision.models import resnet101, ResNet101_Weights\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "torch.hub.set_dir('/local/data1/honzh073/download/TORCH_PRETRAINED')\n",
    "\n",
    "class CustomNet(nn.Module):\n",
    "    def __init__(self, num_classes=2, freeze_percentage=0.5):\n",
    "        super(CustomNet, self).__init__()\n",
    "\n",
    "        self.features = models.resnet101(weights=ResNet101_Weights.DEFAULT)\n",
    "\n",
    "        # Calculate the index where to split the layers\n",
    "        total_layers = len(list(self.features.children()))\n",
    "        split_idx = int(total_layers * freeze_percentage)\n",
    "\n",
    "        # Split the layers into groups for freezing and non-freezing\n",
    "        children = list(self.features.children())\n",
    "        self.frozen_features = nn.Sequential(*children[:split_idx])\n",
    "        self.unfrozen_features = nn.Sequential(*children[split_idx:-1])  # Exclude the last layer\n",
    "\n",
    "        # Freeze layers in self.frozen_features\n",
    "        for param in self.frozen_features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Define the final fully connected layer\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.frozen_features(x)\n",
    "        x = self.unfrozen_features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "# create net\n",
    "model = CustomNet(num_classes=2)  # num_classes 2\n",
    "\n",
    "# checkpoint\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "global_path = '/local/data1/honzh073/data/suzuki.ad.liu.se/fb2580aa-55f7-4a6d-b7d3-47343e1177d1/app_server/FL_global_model.pt'\n",
    "checkpoint = torch.load(global_path, map_location=device)\n",
    "\n",
    "# print(checkpoint.keys())\n",
    "\n",
    "# modify checkpoint keys，match model\n",
    "# load model weights\n",
    "state_dict = checkpoint['model']\n",
    "modified_state_dict = {}\n",
    "for key, value in state_dict.items():\n",
    "    # modify key names，match model layers\n",
    "    new_key = key.replace(\"model.\", \"\")\n",
    "    modified_state_dict[new_key] = value\n",
    "\n",
    "# load weights to model\n",
    "model.load_state_dict(modified_state_dict)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "run_10_times(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test Loss: 0.3648\n",
      "Standard Deviation Test Loss: 0.0624\n",
      "Mean Total Accuracy: 90.255\n",
      "Standard Deviation Total Accuracy: 1.4748\n",
      "Mean NFF Accuracy: 95.3816\n",
      "Standard Deviation NFF Accuracy: 1.0494\n",
      "Mean AFF Accuracy: 63.9582\n",
      "Standard Deviation AFF Accuracy: 4.9979\n",
      "Mean AUC: 0.7967\n",
      "Standard Deviation AUC: 0.0264\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision.models import resnet152, ResNet152_Weights\n",
    "\n",
    "torch.hub.set_dir('/local/data1/honzh073/download/TORCH_PRETRAINED')\n",
    "\n",
    "class CustomNet(nn.Module):\n",
    "    def __init__(self, num_classes=2, freeze_percentage=0.5):\n",
    "        super(CustomNet, self).__init__()\n",
    "\n",
    "        self.features = models.resnet152(weights=ResNet152_Weights.DEFAULT)\n",
    "\n",
    "        # Calculate the index where to split the layers\n",
    "        total_layers = len(list(self.features.children()))\n",
    "        split_idx = int(total_layers * freeze_percentage)\n",
    "\n",
    "        # Split the layers into groups for freezing and non-freezing\n",
    "        children = list(self.features.children())\n",
    "        self.frozen_features = nn.Sequential(*children[:split_idx])\n",
    "        self.unfrozen_features = nn.Sequential(*children[split_idx:-1])  # Exclude the last layer\n",
    "\n",
    "        # Freeze layers in self.frozen_features\n",
    "        for param in self.frozen_features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Define the final fully connected layer\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.frozen_features(x)\n",
    "        x = self.unfrozen_features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "# create net\n",
    "model = CustomNet(num_classes=2)  # num_classes 2\n",
    "\n",
    "# checkpoint\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# resnet152_freeze_Adam_200R_1E\n",
    "global_path = '/local/data1/honzh073/data/suzuki.ad.liu.se/020020f4-41a7-4693-9b42-b6414b1010f8/app_server/FL_global_model.pt'\n",
    "checkpoint = torch.load(global_path, map_location=device)\n",
    "\n",
    "# print(checkpoint.keys())\n",
    "\n",
    "# modify checkpoint keys，match model\n",
    "# load model weights\n",
    "state_dict = checkpoint['model']\n",
    "modified_state_dict = {}\n",
    "for key, value in state_dict.items():\n",
    "    # modify key names，match model layers\n",
    "    new_key = key.replace(\"model.\", \"\")\n",
    "    modified_state_dict[new_key] = value\n",
    "\n",
    "# load weights to model\n",
    "model.load_state_dict(modified_state_dict)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "run_10_times(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet 161\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_100_times' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/local/data1/honzh073/local_repository/FL/code/6_test_global_model/test_global_model/test_100R2E.ipynb Cell 15\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/code/6_test_global_model/test_global_model/test_100R2E.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/code/6_test_global_model/test_global_model/test_100R2E.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/code/6_test_global_model/test_global_model/test_100R2E.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m run_100_times(model)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_100_times' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision.models import densenet161, DenseNet161_Weights\n",
    "\n",
    "torch.hub.set_dir('/local/data1/honzh073/download/TORCH_PRETRAINED')\n",
    "\n",
    "class CustomNet(nn.Module):\n",
    "    def __init__(self, num_classes=2, freeze_percentage=0.5):\n",
    "        super(CustomNet, self).__init__()\n",
    "\n",
    "        self.features = models.densenet161(weights=DenseNet161_Weights.DEFAULT)\n",
    "\n",
    "        # Calculate the index where to split the layers\n",
    "        total_layers = len(list(self.features.children()))\n",
    "        split_idx = int(total_layers * freeze_percentage)\n",
    "\n",
    "        # Split the layers into groups for freezing and non-freezing\n",
    "        children = list(self.features.children())\n",
    "        self.frozen_features = nn.Sequential(*children[:split_idx])\n",
    "        self.unfrozen_features = nn.Sequential(*children[split_idx:-1])  # Exclude the last layer\n",
    "\n",
    "        # Freeze layers in self.frozen_features\n",
    "        for param in self.frozen_features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Define the final fully connected layer\n",
    "        self.fc = nn.Linear(2208, num_classes)  # DenseNet-161 has 2208 output features\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.frozen_features(x)\n",
    "        x = self.unfrozen_features(x)\n",
    "        x = nn.functional.adaptive_avg_pool2d(x, (1, 1))  # Global average pooling\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# create net\n",
    "model = CustomNet(num_classes=2)  # num_classes 2\n",
    "\n",
    "# checkpoint\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# resnet101_freeze_Adam_200R_1E\n",
    "global_path = '/local/data1/honzh073/data/suzuki.ad.liu.se/transfer/densenet161_freeze_Adam_100R2E/workspace/app_server/FL_global_model.pt'\n",
    "\n",
    "checkpoint = torch.load(global_path, map_location=device)\n",
    "\n",
    "# print(checkpoint.keys())\n",
    "\n",
    "# modify checkpoint keys，match model\n",
    "# load model weights\n",
    "state_dict = checkpoint['model']\n",
    "modified_state_dict = {}\n",
    "for key, value in state_dict.items():\n",
    "    # modify key names，match model layers\n",
    "    new_key = key.replace(\"model.\", \"\")\n",
    "    modified_state_dict[new_key] = value\n",
    "\n",
    "# load weights to model\n",
    "model.load_state_dict(modified_state_dict)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "run_100_times(model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
