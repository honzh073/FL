{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# get training class weight, aff : nff = 2:8\n",
    "def get_classweight(train_dataset):\n",
    "    train_nff_count = sum(1 for _, label in train_dataset if label == 0)  # 0 NFF\n",
    "    train_aff_count = sum(1 for _, label in train_dataset if label == 1)  # 1 AFF\n",
    "    class_weight_nff = 1 / (2 * (train_nff_count / (train_nff_count + train_aff_count)))\n",
    "    class_weight_aff = 1 / (2 * (train_aff_count / (train_nff_count + train_aff_count)))\n",
    "    \n",
    "    return [class_weight_nff, class_weight_aff] # 0 nff , 1 aff \n",
    "\n",
    "# training function\n",
    "def train_model(train_loader, validation_loader, classweight, num_epochs, lr, step_size, gamma, model_name):\n",
    "    # Load pre-trained model\n",
    "    torch.hub.set_dir('/local/data1/honzh073/download/TORCH_PRETRAINED')\n",
    "    \n",
    "    if model_name == 'resnet18':\n",
    "        from torchvision.models import resnet18, ResNet18_Weights\n",
    "        model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "    \n",
    "    elif model_name == 'resnet50':\n",
    "        from torchvision.models import resnet50, ResNet50_Weights\n",
    "        model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        \n",
    "    elif model_name == 'resnet101':\n",
    "        from torchvision.models import resnet101, ResNet101_Weights\n",
    "        model = models.resnet101(weights=ResNet101_Weights.DEFAULT)\n",
    "         \n",
    "    elif model_name == 'resnet152':\n",
    "        from torchvision.models import resnet152, ResNet152_Weights\n",
    "        model = models.resnet152(weights=ResNet152_Weights.DEFAULT)\n",
    "        \n",
    "    elif model_name == 'densenet161':\n",
    "        from torchvision.models import densenet161, DenseNet161_Weights\n",
    "        model = models.densenet161(weights=DenseNet161_Weights.DEFAULT)\n",
    "\n",
    "    elif model_name == 'vgg19':\n",
    "        from torchvision.models import vgg19, VGG19_Weights\n",
    "        model = models.vgg19(weights=VGG19_Weights.DEFAULT)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type. 'vgg19' 'resnet50' 'resnet101' 'resnet152' or 'densenet161'.\")\n",
    "    \n",
    "    # freeze all layers except fc\n",
    "    # for param in model.parameters():\n",
    "    #     param.requires_grad = False\n",
    "    \n",
    "    # fc\n",
    "    num_class = 2\n",
    "    \n",
    "    if model_name == 'densenet161':\n",
    "        in_features = model.classifier.in_features\n",
    "        model.classifier = nn.Sequential(nn.Dropout(0.5),nn.Linear(in_features, num_class)) # dropout\n",
    "        # model.classifier = nn.Sequential(nn.Linear(in_features, num_class)) # no dropout\n",
    "    if model_name == 'vgg19':\n",
    "        in_features = model.classifier[0].in_features\n",
    "        model.classifier = nn.Sequential(nn.Dropout(0.5),nn.Linear(in_features, num_class)) # dropout\n",
    "        # model.classifier = nn.Sequential(nn.Linear(in_features, num_class)) # no dropout\n",
    "    # fc1 = vgg_model.classifier[0]\n",
    "\n",
    "    else:\n",
    "        in_features = model.fc.in_features\n",
    "        model.fc = nn.Sequential(nn.Dropout(0.5), nn.Linear(in_features, num_class)) # dropout\n",
    "        # model.fc = nn.Sequential(nn.Linear(in_features, num_class)) # no dropout\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # # DataParallel speed up\n",
    "    # if torch.cuda.device_count() > 1:\n",
    "    #     print(\"multiple GPU:\", torch.cuda.device_count())\n",
    "    #     model = nn.DataParallel(model)\n",
    "    # else:\n",
    "    #     print(\"single GPU\")\n",
    "    #     model = model.to(device)\n",
    "    \n",
    "    # loss function and learning rate\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.Tensor(classweight).to(device))\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
    "\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    # # scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    # scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "        \n",
    "    # Loss, ACC\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    train_accuracies = []\n",
    "    validation_accuracies = []\n",
    "    \n",
    "    # select best model\n",
    "    best_validation_accuracy = 0\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "        \n",
    "        model.eval()\n",
    "        correct_validation_aff = 0\n",
    "        total_validation_aff = 0\n",
    "        correct_validation = 0\n",
    "        total_validation = 0\n",
    "        validation_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in validation_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_validation += labels.size(0)\n",
    "                correct_validation += (predicted == labels).sum().item()\n",
    "                validation_loss += loss.item()\n",
    "                # Calculate accuracy for AFF class\n",
    "                total_validation_aff += torch.sum(labels == 1).item()\n",
    "                correct_validation_aff += torch.sum((predicted == 1) & (labels == 1)).item()\n",
    "\n",
    "        validation_accuracy_aff = 100 * correct_validation_aff / total_validation_aff\n",
    "\n",
    "        # validation accuracy and loss\n",
    "        validation_accuracy = 100 * correct_validation / total_validation\n",
    "        validation_loss /= len(validation_loader)\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        validation_losses.append(validation_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        validation_accuracies.append(validation_accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "            f\"train Loss: {loss.item():.4f}, \"\n",
    "            f\"val Loss: {validation_loss:.4f}, \"\n",
    "            f\"train ACC: {train_accuracy:.2f}%, \"\n",
    "            f\"Val ACC: {validation_accuracy:.2f}%\")\n",
    "        \n",
    "        # scheduler.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # select by high aff ACC\n",
    "        # if validation_accuracy_aff > best_accuracy:\n",
    "        #     best_accuracy = validation_accuracy_aff\n",
    "        #     best_model = model.module if isinstance(model, nn.DataParallel) else model\n",
    "\n",
    "        if validation_accuracy > best_validation_accuracy:\n",
    "            best_validation_accuracy = validation_accuracy\n",
    "            # best_model = model.module  # DataParallel\n",
    "            best_model = model  # single device\n",
    "            \n",
    "    # Plot train/val loss,  accuracy\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(validation_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(validation_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend() \n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def test_model(model, test_dataset, batch_size):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    model.eval()\n",
    "    \n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    test_loss = 0\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    test_accuracy = 100 * correct_test / total_test\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.2f}%\")\n",
    "     \n",
    "    auc_score = roc_auc_score(all_labels, all_predictions)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    # Precision、Recall、F1 Score\n",
    "    class_labels = {0: 'NFF', 1: 'AFF'}\n",
    "\n",
    "    classification_rep = classification_report(all_labels, all_predictions, target_names=[class_labels[i] for i in range(len(class_labels))])\n",
    "    print(\"AUC:\", auc_score)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_rep)\n",
    "\n",
    "    plot_roc_curve(all_labels, all_predictions)\n",
    "    \n",
    "def plot_roc_curve(all_labels, all_predictions):\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_predictions)\n",
    "    roc_auc = roc_auc_score(all_labels, all_predictions)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "def show_image(dataset, num_images=5):\n",
    "    # Get random indices\n",
    "    random_indices = np.random.choice(len(dataset), num_images, replace=False)\n",
    "\n",
    "    # Plot images with truncated names\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, idx in enumerate(random_indices):\n",
    "        image, label = dataset[idx]  # Use the dataset directly\n",
    "        filename = dataset.data[idx][0]  # Get the filename from dataset's internal data attribute\n",
    "        truncated_filename = filename.split('/')[-1][:15]  # Extract the last part and truncate to 15 characters\n",
    "        \n",
    "        # Print the original filename\n",
    "        print(f\"Image location: {filename}\")\n",
    "\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.title(f\"Label: {label}\")\n",
    "        plt.imshow(image[0])  # Assuming single-channel (grayscale) image\n",
    "        # plt.imshow(image[0], cmap='gray')  # Assuming single-channel (grayscale) image\n",
    "\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# import os\n",
    "# from collections import defaultdict\n",
    "\n",
    "# def filter_hospital_data(input_csv_path, target_hospital_ids):\n",
    "#     patient_data = defaultdict(list)\n",
    "\n",
    "#     with open(input_csv_path, 'r') as csvfile:\n",
    "#         reader = csv.DictReader(csvfile)\n",
    "#         for row in reader:\n",
    "#             if row['HospitalID'] in target_hospital_ids:\n",
    "#                 patient_id = row['PatientID']\n",
    "#                 patient_data[patient_id].append(row)\n",
    "\n",
    "#     hospital_55_data = []\n",
    "#     for images in patient_data.values():\n",
    "#         hospital_55_data.extend(images)\n",
    "\n",
    "#     return hospital_55_data\n",
    "\n",
    "# # Input and output paths\n",
    "# input_csv_path = \"/local/data1/honzh073/local_repository/FL/code/3_single_hospital/csv_files/image_data.csv\"\n",
    "# output_folder = '/local/data1/honzh073/local_repository/FL/code/3_single_hospital/csv_files/'\n",
    "\n",
    "# # Single hospital id\n",
    "# target_hospital_ids = ['55']\n",
    "\n",
    "# # Get data for hospital 55\n",
    "# hospital_55_data = filter_hospital_data(input_csv_path, target_hospital_ids)\n",
    "\n",
    "# # Write hospital 55 data to CSV file\n",
    "# def write_to_csv(file_path, data):\n",
    "#     with open(file_path, 'w', newline='') as csvfile:\n",
    "#         writer = csv.DictWriter(csvfile, fieldnames=data[0].keys())\n",
    "#         writer.writeheader()\n",
    "#         writer.writerows(data)\n",
    "\n",
    "# # Save hospital 55 data to 'hospital55.csv'\n",
    "# write_to_csv(os.path.join(output_folder, 'hospital55.csv'), hospital_55_data)\n",
    "\n",
    "# print(\"Saved hospital55.csv for hospital 55.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: AFF count: 49, NFF count: 123\n",
      "Validation dataset: AFF count: 26, NFF count: 47\n",
      "[0.6991869918699186, 1.7551020408163265]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import json\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image_path = item['image_path']\n",
    "        label = item['label']\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert label to numerical representation\n",
    "        if label == 'NFF':\n",
    "            label = 0\n",
    "        elif label == 'AFF':\n",
    "            label = 1\n",
    "        else:\n",
    "            raise ValueError(\"Invalid label in JSON data.\")\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Define data augmentation transforms for training data\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=30),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomResizedCrop(256, scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.GaussianBlur(kernel_size=3),\n",
    "    transforms.RandomAdjustSharpness(sharpness_factor=5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load JSON data\n",
    "with open('/local/data1/honzh073/data/suzuki.json', 'r') as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "    train_data = json_data['train']\n",
    "    val_data = json_data['val']\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset = CustomDataset(train_data, transform=train_transform)\n",
    "val_dataset = CustomDataset(val_data, transform=test_transform)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Count 'AFF' and 'NFF' labels in train dataset\n",
    "train_aff_count = sum(1 for _, label in train_dataset if label == 1)  # 'AFF' label is 1\n",
    "train_nff_count = sum(1 for _, label in train_dataset if label == 0)  # 'NFF' label is 0\n",
    "\n",
    "# Count 'AFF' and 'NFF' labels in validation dataset\n",
    "val_aff_count = sum(1 for _, label in val_dataset if label == 1)  # 'AFF' label is 1\n",
    "val_nff_count = sum(1 for _, label in val_dataset if label == 0)  # 'NFF' label is 0\n",
    "\n",
    "print(f\"Train dataset: AFF count: {train_aff_count}, NFF count: {train_nff_count}\")\n",
    "print(f\"Validation dataset: AFF count: {val_aff_count}, NFF count: {val_nff_count}\")\n",
    "classweight = get_classweight(train_dataset)\n",
    "print(classweight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = 50\n",
    "step_size = 10\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# resnet101\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m resnet101 \u001b[39m=\u001b[39m train_model(train_loader, val_loader, classweight, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m                         num_epochs\u001b[39m=\u001b[39;49mepoch_num, lr\u001b[39m=\u001b[39;49mlr, step_size\u001b[39m=\u001b[39;49mstep_size, gamma\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m, model_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mresnet101\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=108'>109</a>\u001b[0m correct_train \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=109'>110</a>\u001b[0m total_train \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=111'>112</a>\u001b[0m \u001b[39mfor\u001b[39;00m images, labels \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=112'>113</a>\u001b[0m     images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=113'>114</a>\u001b[0m     labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/local/data1/honzh073/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/local/data1/honzh073/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/local/data1/honzh073/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/local/data1/honzh073/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(image_path)\u001b[39m.\u001b[39mconvert(\u001b[39m'\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(image)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Convert label to numerical representation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mif\u001b[39;00m label \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mNFF\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m/local/data1/honzh073/anaconda3/envs/myenv/lib/python3.9/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/local/data1/honzh073/anaconda3/envs/myenv/lib/python3.9/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[0;32m/local/data1/honzh073/anaconda3/envs/myenv/lib/python3.9/site-packages/torchvision/transforms/functional.py:172\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    170\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mview(pic\u001b[39m.\u001b[39msize[\u001b[39m1\u001b[39m], pic\u001b[39m.\u001b[39msize[\u001b[39m0\u001b[39m], F_pil\u001b[39m.\u001b[39mget_image_num_channels(pic))\n\u001b[1;32m    171\u001b[0m \u001b[39m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39;49mpermute((\u001b[39m2\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m))\u001b[39m.\u001b[39;49mcontiguous()\n\u001b[1;32m    173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mByteTensor):\n\u001b[1;32m    174\u001b[0m     \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39mdefault_float_dtype)\u001b[39m.\u001b[39mdiv(\u001b[39m255\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# resnet101\n",
    "resnet101 = train_model(train_loader, val_loader, classweight, \n",
    "                        num_epochs=epoch_num, lr=lr, step_size=step_size, gamma=0.1, model_name='resnet101')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'fl' from 'nvflare.apis' (/local/data1/honzh073/anaconda3/envs/nvflare-env/lib/python3.9/site-packages/nvflare/apis/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb Cell 6\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnvflare\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnvflare\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapis\u001b[39;00m \u001b[39mimport\u001b[39;00m fl\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# 定义你的数据集类\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCustomDataset\u001b[39;00m(Dataset):\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'fl' from 'nvflare.apis' (/local/data1/honzh073/anaconda3/envs/nvflare-env/lib/python3.9/site-packages/nvflare/apis/__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import json\n",
    "import nvflare\n",
    "from nvflare.apis import fl\n",
    "\n",
    "# 定义你的数据集类\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image_path = item['image_path']\n",
    "        label = item['label']\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert label to numerical representation\n",
    "        if label == 'NFF':\n",
    "            label = 0\n",
    "        elif label == 'AFF':\n",
    "            label = 1\n",
    "        else:\n",
    "            raise ValueError(\"Invalid label in JSON data.\")\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# 定义数据转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 加载JSON数据到两个不同的本地设备\n",
    "device0 = torch.device(\"cuda:0\")\n",
    "device1 = torch.device(\"cuda:1\")\n",
    "\n",
    "# 读取suzuki.json文件到cuda:0\n",
    "with open('/local/data1/honzh073/data/suzuki.json', 'r') as json_file:\n",
    "    json_data_0 = json.load(json_file)\n",
    "    train_data_0 = json_data_0['train']\n",
    "    val_data_0 = json_data_0['val']\n",
    "\n",
    "# 读取hospital54_18.json文件到cuda:1\n",
    "with open('/local/data1/honzh073/local_repository/FL/learner_json/hospital54_18.json', 'r') as json_file:\n",
    "    json_data_1 = json.load(json_file)\n",
    "    train_data_1 = json_data_1['train']\n",
    "    val_data_1 = json_data_1['val']\n",
    "\n",
    "# 创建本地数据集\n",
    "train_dataset_0 = CustomDataset(train_data_0, transform=transform)\n",
    "val_dataset_0 = CustomDataset(val_data_0, transform=transform)\n",
    "train_dataset_1 = CustomDataset(train_data_1, transform=transform)\n",
    "val_dataset_1 = CustomDataset(val_data_1, transform=transform)\n",
    "\n",
    "# 在本地设备上创建数据加载器\n",
    "batch_size = 64\n",
    "train_loader_0 = DataLoader(train_dataset_0, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_loader_0 = DataLoader(val_dataset_0, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "train_loader_1 = DataLoader(train_dataset_1, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_loader_1 = DataLoader(val_dataset_1, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "# 在每个本地设备上定义模型\n",
    "model_0 = models.resnet101(pretrained=True).to(device0)\n",
    "model_1 = models.resnet101(pretrained=True).to(device1)\n",
    "\n",
    "# 在每个本地设备上定义损失函数和优化器\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer_0 = torch.optim.SGD(model_0.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer_1 = torch.optim.SGD(model_1.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# 联邦学习的迭代次数\n",
    "num_epochs = 10\n",
    "\n",
    "# 开始联邦学习的训练\n",
    "for epoch in range(num_epochs):\n",
    "    # 在第一个本地设备上进行训练\n",
    "    model_0.train()\n",
    "    for local_batch, (local_x, local_y) in enumerate(train_loader_0):\n",
    "        local_x, local_y = local_x.to(device0), local_y.to(device0)\n",
    "        optimizer_0.zero_grad()\n",
    "        local_outputs = model_0(local_x)\n",
    "        loss = criterion(local_outputs, local_y)\n",
    "        loss.backward()\n",
    "        optimizer_0.step()\n",
    "\n",
    "    # 在第二个本地设备上进行训练\n",
    "    model_1.train()\n",
    "    for local_batch, (local_x, local_y) in enumerate(train_loader_1):\n",
    "        local_x, local_y = local_x.to(device1), local_y.to(device1)\n",
    "        optimizer_1.zero_grad()\n",
    "        local_outputs = model_1(local_x)\n",
    "        loss = criterion(local_outputs, local_y)\n",
    "        loss.backward()\n",
    "        optimizer_1.step()\n",
    "\n",
    "    # 将本地模型上传到服务器并进行全局模型更新\n",
    "    fl.fedavg([model_0, model_1])\n",
    "\n",
    "    # 在验证集上测试全局模型的性能（这一步是在本地设备上进行的）\n",
    "    model_0.eval()\n",
    "    model_1.eval()\n",
    "\n",
    "    correct_0, total_0 = 0, 0\n",
    "    correct_1, total_1 = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for local_x, local_y in val_loader_0:\n",
    "            local_x, local_y = local_x.to(device0), local_y.to(device0)\n",
    "            outputs = model_0(local_x)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_0 += local_y.size(0)\n",
    "            correct_0 += (predicted == local_y).sum().item()\n",
    "\n",
    "        for local_x, local_y in val_loader_1:\n",
    "            local_x, local_y = local_x.to(device1), local_y.to(device1)\n",
    "            outputs = model_1(local_x)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_1 += local_y.size(0)\n",
    "            correct_1 += (predicted == local_y).sum().item()\n",
    "\n",
    "    accuracy_0 = 100 * correct_0 / total_0\n",
    "    accuracy_1 = 100 * correct_1 / total_1\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Validation Accuracy on cuda:0: {accuracy_0:.2f}%, Validation Accuracy on cuda:1: {accuracy_1:.2f}%')\n",
    "\n",
    "# 关闭本地设备\n",
    "fl.stop_device(device0)\n",
    "fl.stop_device(device1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet 101\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m test_dataset \u001b[39m=\u001b[39m CustomDataset(\u001b[39m'\u001b[39m\u001b[39m/local/data1/honzh073/local_repository/FL/code/3_single_hospital/csv_files/hospital55.csv\u001b[39m\u001b[39m'\u001b[39m, transform\u001b[39m=\u001b[39mtest_transform)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# For test dataset\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m test_NFF_count \u001b[39m=\u001b[39m \u001b[39msum\u001b[39;49m(\u001b[39m1\u001b[39;49m \u001b[39mfor\u001b[39;49;00m _, label \u001b[39min\u001b[39;49;00m test_dataset \u001b[39mif\u001b[39;49;00m label \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m)  \u001b[39m# 0 NFF\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m test_AFF_count \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(\u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m _, label \u001b[39min\u001b[39;00m test_dataset \u001b[39mif\u001b[39;00m label \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m)  \u001b[39m# 1 AFF\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest AFF: \u001b[39m\u001b[39m{\u001b[39;00mtest_AFF_count\u001b[39m}\u001b[39;00m\u001b[39m, ratio: \u001b[39m\u001b[39m{\u001b[39;00mtest_AFF_count\u001b[39m \u001b[39m\u001b[39m/\u001b[39m\u001b[39m \u001b[39m(test_AFF_count\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39mtest_NFF_count)\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m test_dataset \u001b[39m=\u001b[39m CustomDataset(\u001b[39m'\u001b[39m\u001b[39m/local/data1/honzh073/local_repository/FL/code/3_single_hospital/csv_files/hospital55.csv\u001b[39m\u001b[39m'\u001b[39m, transform\u001b[39m=\u001b[39mtest_transform)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# For test dataset\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m test_NFF_count \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(\u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m _, label \u001b[39min\u001b[39;00m test_dataset \u001b[39mif\u001b[39;00m label \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)  \u001b[39m# 0 NFF\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m test_AFF_count \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(\u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m _, label \u001b[39min\u001b[39;00m test_dataset \u001b[39mif\u001b[39;00m label \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m)  \u001b[39m# 1 AFF\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest AFF: \u001b[39m\u001b[39m{\u001b[39;00mtest_AFF_count\u001b[39m}\u001b[39;00m\u001b[39m, ratio: \u001b[39m\u001b[39m{\u001b[39;00mtest_AFF_count\u001b[39m \u001b[39m\u001b[39m/\u001b[39m\u001b[39m \u001b[39m(test_AFF_count\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39mtest_NFF_count)\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[idx]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     image_path \u001b[39m=\u001b[39m item[\u001b[39m'\u001b[39;49m\u001b[39mimage_path\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     label \u001b[39m=\u001b[39m item[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuzuki_ssh/local/data1/honzh073/local_repository/FL/simple_fl/json_train_val.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m# Load image\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "print('ResNet 101')\n",
    "test_dataset = CustomDataset('/local/data1/honzh073/local_repository/FL/code/3_single_hospital/csv_files/hospital55.csv', transform=test_transform)\n",
    "# For test dataset\n",
    "test_NFF_count = sum(1 for _, label in test_dataset if label == 0)  # 0 NFF\n",
    "test_AFF_count = sum(1 for _, label in test_dataset if label == 1)  # 1 AFF\n",
    "print(f\"test AFF: {test_AFF_count}, ratio: {test_AFF_count / (test_AFF_count + test_NFF_count):.2f}\")\n",
    "print(f\"---- NFF: {test_NFF_count}, ratio: {test_NFF_count / (test_AFF_count + test_NFF_count):.2f}\")\n",
    "\n",
    "test_model(model=resnet101, test_dataset=test_dataset, batch_size=batch_size)\n",
    "params_count = count_parameters(resnet101)\n",
    "print(f\"number of parameters: {params_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # resnet152\n",
    "# resnet152 = train_model(train_loader, val_loader, classweight, \n",
    "#                         num_epochs=epoch_num, lr=lr, step_size=step_size, gamma=0.1, model_name='resnet152')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('ResNet 152')\n",
    "\n",
    "# test_model(model=resnet152, test_dataset=test_dataset, batch_size=batch_size)\n",
    "# params_count = count_parameters(resnet152)\n",
    "# print(f\"number of parameters: {params_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# densenet161\n",
    "\n",
    "# densenet161 = train_model(train_loader, val_loader, classweight, \n",
    "#                           num_epochs=epoch_num, lr=lr, step_size=step_size, gamma=0.1, model_name='densenet161')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model(model=densenet161, test_dataset=test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # vgg19\n",
    "# vgg19 = train_model(train_loader, val_loader, classweight, \n",
    "#                           num_epochs=epoch_num, lr=lr, step_size=step_size, gamma=0.1, model_name='vgg19')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model(model=vgg19, test_dataset=test_dataset, batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
